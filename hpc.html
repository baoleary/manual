<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>2. HPC Services &mdash; Cloud Computing Book 0.1 documentation</title>
    
    <link rel="stylesheet" href="_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/bootstrap-2.3.2/css/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="http://netdna.bootstrapcdn.com/bootswatch/2.3.2/cosmo/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="_static/bootstrap-2.3.2/css/bootstrap-responsive.min.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="_static/js/jquery-1.9.1.min.js"></script>
    <script type="text/javascript" src="_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="_static/bootstrap-2.3.2/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="_static/bootstrap-sphinx.js"></script>
    <link rel="top" title="Cloud Computing Book 0.1 documentation" href="index.html" />
    <link rel="next" title="3. ScaleMP vSMP" href="scalemp.html" />
    <link rel="prev" title="1. Hardware" href="hardware.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body>

  <div id="navbar" class="navbar navbar-fixed-top">
    <div class="navbar-inner">
      <div class="container">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>

        <a class="brand" href="index.html">Contents</a>
        <span class="navbar-text pull-left"><b>0.1</b></span>

        <div class="nav-collapse">
          <ul class="nav">
            <li class="divider-vertical"></li>
            
              <li class="dropdown globaltoc-container">
  <a href="index.html"
     class="dropdown-toggle"
     data-toggle="dropdown">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
    ><ul>
<li class="toctree-l1"><a class="reference internal" href="preface.html">1. Preface</a><ul>
<li class="toctree-l2"><a class="reference internal" href="preface.html#citation-for-publications">1.1. Citation for Publications</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface.html#acknowledgement">1.2. Acknowledgement</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface.html#sponsors">1.3. Sponsors</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface.html#about-this-manual">1.4. About this Manual</a></li>
<li class="toctree-l2"><a class="reference internal" href="preface.html#conventions">1.5. Conventions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">2. Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#executive-summary">2.1. Executive Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#project-and-account-application">2.2. Project and Account Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#services">2.3. Services</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#hardware">2.4. Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="introduction.html#support">2.5. Support</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="account.html">1. Project and Account Management</a><ul>
<li class="toctree-l2"><a class="reference internal" href="account.html#terminology">1.1. Terminology</a></li>
<li class="toctree-l2"><a class="reference internal" href="account.html#quickstart">1.2. Quickstart</a></li>
<li class="toctree-l2"><a class="reference internal" href="account.html#projects-and-accounts-for-xsede-users">1.3. Projects and Accounts for XSEDE users</a></li>
<li class="toctree-l2"><a class="reference internal" href="account.html#project-management">1.4. Project Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="account.html#upload-a-ssh-public-key">1.5. Upload a SSH Public Key</a></li>
<li class="toctree-l2"><a class="reference internal" href="account.html#upload-an-openid">1.6. Upload an OpenId</a></li>
<li class="toctree-l2"><a class="reference internal" href="account.html#accessing-futuregrid-resources">1.7. Accessing FutureGrid Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="account.html#manage-a-class-on-futuregrid">1.8. Manage a Class on FutureGrid</a></li>
<li class="toctree-l2"><a class="reference internal" href="account.html#mini-faq">1.9. Mini FAQ</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="security.html">2. Using SSH keys</a><ul>
<li class="toctree-l2"><a class="reference internal" href="security.html#using-ssh-from-windows">2.1. Using SSH from Windows</a></li>
<li class="toctree-l2"><a class="reference internal" href="security.html#generate-a-ssh-key">2.2. Generate a SSH key</a></li>
<li class="toctree-l2"><a class="reference internal" href="security.html#add-or-replace-passphrase-for-an-already-generated-key">2.3. Add or Replace Passphrase for an Already Generated Key</a></li>
<li class="toctree-l2"><a class="reference internal" href="security.html#upload-the-key-to-the-futuregrid-portal">2.4. Upload the key to the FutureGrid Portal</a></li>
<li class="toctree-l2"><a class="reference internal" href="security.html#testing-your-ssh-key">2.5. Testing your ssh key</a></li>
<li class="toctree-l2"><a class="reference internal" href="security.html#testing-your-ssh-key-for-hotel">2.6. Testing your ssh key for Hotel</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="status.html">1. Status</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="hardware.html">1. Hardware</a><ul>
<li class="toctree-l2"><a class="reference internal" href="hardware.html#compute-resources">1.1. Compute Resources</a></li>
<li class="toctree-l2"><a class="reference internal" href="hardware.html#networks">1.2. Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="hardware.html#network-impairments-device-nid">1.3. Network Impairments Device (NID)</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="">2. HPC Services</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#login-nodes">2.1. Login Nodes</a></li>
<li class="toctree-l2"><a class="reference internal" href="#message-passing-interface-mpi">2.2. Message Passing Interface (MPI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#job-management">2.3. Job Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="#xray-hpc-services">2.4. Xray HPC Services</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="scalemp.html">3. ScaleMP vSMP</a><ul>
<li class="toctree-l2"><a class="reference internal" href="scalemp.html#accessing-scalemp">3.1. Accessing ScaleMP</a></li>
<li class="toctree-l2"><a class="reference internal" href="scalemp.html#submitting-a-job">3.2. Submitting a job</a></li>
<li class="toctree-l2"><a class="reference internal" href="scalemp.html#developing-a-job-script">3.3. Developing a job script</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="iaas.html">1. IaaS</a><ul>
<li class="toctree-l2"><a class="reference internal" href="iaas.html#nimbus-clouds">1.1. Nimbus Clouds</a></li>
<li class="toctree-l2"><a class="reference internal" href="iaas.html#openstack-clouds">1.2. OpenStack Clouds</a></li>
<li class="toctree-l2"><a class="reference internal" href="iaas.html#eucalyptus-clouds">1.3. Eucalyptus Clouds</a></li>
<li class="toctree-l2"><a class="reference internal" href="iaas.html#virtual-appliances-for-training-and-education">1.4. Virtual Appliances for Training and Education</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="openstack.html">2. OpenStack Essex with euca2ools</a><ul>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#prerequisites">2.1. Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#log-into-india">2.2. Log into India</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#system-variable-user">2.3. System Variable $USER</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#account-and-credentials">2.4. Account and Credentials</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#euca2ools-ec2-client-tools">2.5. Euca2ools (EC2 client tools)</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#testing-your-setup">2.6. Testing Your Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#list-of-common-images">2.7. List of Common Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#vm-types">2.8. VM Types</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#key-management">2.9. Key Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#image-instantiation">2.10. Image Instantiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#rename-server-names">2.11. Rename Server Names</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#monitoring-instances">2.12. Monitoring Instances</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#log-into-your-vm">2.13. Log into your VM</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#nova-volumes-not-available">2.14. Nova Volumes (Not available)</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#volume-snapshots">2.15. Volume Snapshots</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#image-registration">2.16. Image Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#delete-your-images">2.17. Delete your images</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#terminate-your-vms">2.18. Terminate your VMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#limitations">2.19. Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#troubleshooting">2.20. Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack.html#compatibility-between-nova-and-euca2ools-commands">2.21. Compatibility between nova and euca2ools commands</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="openstack-grizzly.html">3. OpenStack Grizzly</a><ul>
<li class="toctree-l2"><a class="reference internal" href="openstack-grizzly.html#use-block-storage">3.1. Use Block Storage</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack-grizzly.html#set-up-external-access-to-your-instance">3.2. Set up external access to your instance</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack-grizzly.html#make-a-snapshot-of-an-instance">3.3. Make a snapshot of an instance</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack-grizzly.html#automate-some-initial-configuration">3.4. Automate some initial configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack-grizzly.html#get-the-latest-version-of-ubuntu-cloud-image-and-upload-it-to-the-openstack">3.5. Get the latest version of Ubuntu Cloud Image and upload it to the OpenStack</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack-grizzly.html#delete-your-instance">3.6. Delete your instance</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack-grizzly.html#how-to-change-your-password">3.7. How to change your password</a></li>
<li class="toctree-l2"><a class="reference internal" href="openstack-grizzly.html#things-to-do-when-you-need-euca2ools-or-ec2-interfaces">3.8. Things to do when you need Euca2ools or EC2 interfaces</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="eucalyptus.html">5. Eucalyptus</a><ul>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#account-creation">5.1. Account Creation</a></li>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#resources-overview">5.2. Resources Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#testing-your-setup">5.3. Testing Your Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#image-deployment">5.4. Image Deployment</a></li>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#logging-into-the-vm">5.5. Logging Into the VM</a></li>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#vm-network-info">5.6. VM Network Info</a></li>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#image-management">5.7. Image Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="eucalyptus.html#status-of-deployments">5.8. Status of Deployments</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="rain.html">4. RAIN</a><ul>
<li class="toctree-l2"><a class="reference internal" href="rain.html#generate-and-register-an-os-image-on-futuregrid-using-the-fg-shell">4.1. Generate and Register an OS Image on FutureGrid using the FG Shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="rain.html#futuregrid-standalone-image-repository">4.2. FutureGrid Standalone Image Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="rain.html#manual-image-customization">4.3. Manual Image Customization</a></li>
<li class="toctree-l2"><a class="reference internal" href="rain.html#rain-manual-pages">4.4. RAIN Manual Pages</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="mooc/accounts.html">1. Screencast: Account Creation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="mooc/accounts.html#overview">1.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="mooc/accounts.html#prerequisites">1.2. Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="mooc/accounts.html#instructor-bio">1.3. Instructor Bio</a></li>
<li class="toctree-l2"><a class="reference internal" href="mooc/accounts.html#faq">1.4. FAQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="mooc/accounts.html#course-content">1.5. Course Content</a></li>
<li class="toctree-l2"><a class="reference internal" href="mooc/accounts.html#exercises">1.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="mooc/accounts.html#example-project">1.7. Example Project:</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="todolist.html">1. Todo List</a></li>
<li class="toctree-l1"><a class="reference internal" href="plan.html">2. Plan</a></li>
<li class="toctree-l1"><a class="reference internal" href="git.html">13. Git</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">11. Building the Manual</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#python">11.1. Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#virtualenv">11.2. Virtualenv</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#create-a-github-local-directory-with-the-manual">11.3. Create a github local directory with the manual</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#install-the-requirements">11.4. Install the Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#all-in-one-setup-script">11.5. All-in-one setup script</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#watchdog">11.6. Watchdog</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#transfering-a-page-from-the-portal-to-rst">11.7. Transfering a page from the portal to RST</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#creating-the-pages-locally">11.8. Creating the pages locally</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#publishing-the-pages">11.9. Publishing the pages</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#jira">11.10. jira</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#portal-link">11.11. Portal link</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#screencast-recording-tips">11.12. Screencast recording tips</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="mooc.html">5. Template: Module/Unit Title</a><ul>
<li class="toctree-l2"><a class="reference internal" href="mooc.html#overview">5.1. Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="mooc.html#prerequisites">5.2. Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="mooc.html#instructor-bio">5.3. Instructor Bio</a></li>
<li class="toctree-l2"><a class="reference internal" href="mooc.html#faq">5.4. FAQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="mooc.html#course-content">5.5. Course Content</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="ring.html">1. MPI Ring Program</a></li>
</ul>
</ul>
</li>
              <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"><ul>
<li><a class="reference internal" href="#">2. HPC Services</a><ul>
<li><a class="reference internal" href="#login-nodes">2.1. Login Nodes</a><ul>
<li><a class="reference internal" href="#modules">2.1.1. Modules</a></li>
<li><a class="reference internal" href="#list-of-available-modules-on-various-machines">2.1.2. List of Available Modules on Various Machines</a></li>
<li><a class="reference internal" href="#filesystem-layout">2.1.3. Filesystem Layout</a></li>
</ul>
</li>
<li><a class="reference internal" href="#message-passing-interface-mpi">2.2. Message Passing Interface (MPI)</a><ul>
<li><a class="reference internal" href="#mpi-libraries">2.2.1. MPI Libraries</a></li>
<li><a class="reference internal" href="#compiling-mpi-applications">2.2.2. Compiling MPI Applications</a></li>
<li><a class="reference internal" href="#batch-jobs">2.2.3. Batch Jobs</a></li>
</ul>
</li>
<li><a class="reference internal" href="#job-management">2.3. Job Management</a><ul>
<li><a class="reference internal" href="#job-submission">2.3.1. Job Submission</a></li>
<li><a class="reference internal" href="#job-deletion">2.3.2. Job Deletion</a></li>
<li><a class="reference internal" href="#job-monitoring">2.3.3. Job Monitoring</a></li>
<li><a class="reference internal" href="#job-output">2.3.4. Job Output</a></li>
</ul>
</li>
<li><a class="reference internal" href="#xray-hpc-services">2.4. Xray HPC Services</a><ul>
<li><a class="reference internal" href="#submitting-a-job-on-xray">2.4.1. Submitting a Job on xray</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</ul>
</li>
            
            
              
  <li><a href="hardware.html"
         title="previous chapter">&laquo; 1. Hardware</a></li>
  <li><a href="scalemp.html"
         title="next chapter">3. ScaleMP vSMP &raquo;</a></li>
            
            
              <li>
  <a href="_sources/hpc.txt"
     rel="nofollow">Source</a></li>
            
          </ul>

          
            
<form class="navbar-search pull-right" action="search.html" method="get">
  <input type="text" name="q" class="search-query" placeholder="Search" />
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
      </div>
    </div>
  </div>

<div class="container">
  
  <div class="section" id="hpc-services">
<span id="s-hpc"></span><h1>2. HPC Services<a class="headerlink" href="#hpc-services" title="Permalink to this headline">¶</a></h1>
<div class="sidebar">
<p class="first sidebar-title">Page Contents</p>
<div class="contents local last topic" id="contents">
<ul class="simple">
<li><a class="reference internal" href="#login-nodes" id="id3">Login Nodes</a><ul>
<li><a class="reference internal" href="#modules" id="id4">Modules</a></li>
<li><a class="reference internal" href="#list-of-available-modules-on-various-machines" id="id5">List of Available Modules on Various Machines</a></li>
<li><a class="reference internal" href="#filesystem-layout" id="id6">Filesystem Layout</a></li>
</ul>
</li>
<li><a class="reference internal" href="#message-passing-interface-mpi" id="id7">Message Passing Interface (MPI)</a><ul>
<li><a class="reference internal" href="#mpi-libraries" id="id8">MPI Libraries</a></li>
<li><a class="reference internal" href="#compiling-mpi-applications" id="id9">Compiling MPI Applications</a></li>
<li><a class="reference internal" href="#batch-jobs" id="id10">Batch Jobs</a></li>
</ul>
</li>
<li><a class="reference internal" href="#job-management" id="id11">Job Management</a><ul>
<li><a class="reference internal" href="#job-submission" id="id12">Job Submission</a></li>
<li><a class="reference internal" href="#job-deletion" id="id13">Job Deletion</a></li>
<li><a class="reference internal" href="#job-monitoring" id="id14">Job Monitoring</a></li>
<li><a class="reference internal" href="#job-output" id="id15">Job Output</a></li>
</ul>
</li>
<li><a class="reference internal" href="#xray-hpc-services" id="id16">Xray HPC Services</a><ul>
<li><a class="reference internal" href="#submitting-a-job-on-xray" id="id17">Submitting a Job on xray</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="login-nodes">
<span id="s-hpc-access"></span><h2><a class="toc-backref" href="#id3">2.1. Login Nodes</a><a class="headerlink" href="#login-nodes" title="Permalink to this headline">¶</a></h2>
<p>Several of the clusters have High Performance Computing (HPC) services
installed. Access to them is provided via a Linux Login node for each
of the clusters on which these services are installed.</p>
<p>To access the login nodes you need a FG resource account and an SSH
public key you have uploaded to FutureGrid (this process is described
in the section about <a class="reference internal" href="account.html#s-accounts"><em>Project and Account Management</em></a>. After you are part of a valid
project and have a FutureGrid account, you can log into the FutureGrid
resources with ssh. The resources include the following login nodes:</p>
<ul class="simple">
<li>alamo.futuregrid.org</li>
<li>bravo.futuregrid.org</li>
<li>foxtrot.futuregrid.org</li>
<li>hotel.futuregrid.org</li>
<li>india.futuregrid.org</li>
<li>sierra.futuregrid.org</li>
<li>xray.futuregrid.org</li>
</ul>
<div class="admonition-todo admonition" id="index-0">
<p class="first admonition-title">Todo</p>
<p class="last">what are login nodes for delta, echo</p>
</div>
<p>For example, assume your portalname is &#8220;portalname&#8221;, than you can
login to sierra as follows:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>ssh portalname@sierra.futuregrid.org
Welcome to sierra.futuregrid.org
Last login: Thu Aug 12 19:19:22 2010 from ....
</pre></div>
</div>
<div class="section" id="modules">
<h3><a class="toc-backref" href="#id4">2.1.1. Modules</a><a class="headerlink" href="#modules" title="Permalink to this headline">¶</a></h3>
<p>The login nodes have the <a class="reference external" href="http://modules.sourceforge.net">modules</a>
package installed. It provides a convenient tool to adapt your
environment and enables you to activate different packages and services
dependent on your specific needs. The Modules utility to let you
dynamically control your environment. Modules allows you to load and
unload packages and ensure a coherent working environment.
This ensures that your $PATH, $LD_LIBRARY_PATH, $LD_PRELOAD, and other
environment variables are properly set, and that you can access the
programs and libraries you need. For additional information about the
Modules package you can consult the <a class="reference external" href="http://modules.sourceforge.net/man/module.html">manual page</a>.</p>
<p>To display the list of available modules:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>module avail
</pre></div>
</div>
<p>To display the list of currently loaded modules:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>module list
</pre></div>
</div>
<p>To add and remove packages from your environment you can use the
<em>module load</em> and <em>module unload</em> commands:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>module load &lt;package name&gt;/&lt;optional package version&gt;
<span class="nv">$ </span>module unload &lt;package name&gt;/&lt;optional package version&gt;
</pre></div>
</div>
<p>The available command are listed in the next table:</p>
<table border="1" class="docutils">
<caption>Module commands</caption>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Command</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>module avail</td>
<td>List all software packages available on the system.</td>
</tr>
<tr class="row-odd"><td>module avail package</td>
<td>List all versions of package available on the system</td>
</tr>
<tr class="row-even"><td>module list</td>
<td>List all packages currently loaded in your environment.</td>
</tr>
<tr class="row-odd"><td>module load package/version</td>
<td>Add the specified version of the package to your environment</td>
</tr>
<tr class="row-even"><td>module unload package</td>
<td>Remove the specified package from your environment.</td>
</tr>
<tr class="row-odd"><td>module swap package_A package_B</td>
<td>Swap the loaded package (package_A) with another package (package_B).</td>
</tr>
<tr class="row-even"><td>module show package</td>
<td>Shows what changes will be made to your environment (e.g. paths to libraries and executables) by loading the specified package.</td>
</tr>
</tbody>
</table>
<p><strong>Example</strong> - List the currently loaded modules on sierra after login:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>module list

Currently Loaded Modulefiles:
  1<span class="o">)</span> torque/2.4.8   2<span class="o">)</span> moab/5.4.0
</pre></div>
</div>
<p><strong>Example</strong> - list the avialable modules on sierra:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>module avail

----------------- /opt/Modules/3.2.8/modulefiles/applications ------------------
R/2.11.1<span class="o">(</span>default<span class="o">)</span>      hpcc/1.3.1<span class="o">(</span>default<span class="o">)</span>    velvet/1.0.15
git/1.7.10             ncbi/2.2.23<span class="o">(</span>default<span class="o">)</span>   wgs/6.1
gromacs/4.0.7<span class="o">(</span>default<span class="o">)</span> soapdenovo/1.04

------------------- /opt/Modules/3.2.8/modulefiles/compilers -------------------
cmake/2.8.1<span class="o">(</span>default<span class="o">)</span>       java/1.6.0-i586
intel/10.1                 java/1.6.0-x86_64<span class="o">(</span>default<span class="o">)</span>
intel/11.1<span class="o">(</span>default<span class="o">)</span>

------------------- /opt/Modules/3.2.8/modulefiles/debuggers -------------------
null                       totalview/8.8.0-2<span class="o">(</span>default<span class="o">)</span>

------------------- /opt/Modules/3.2.8/modulefiles/libraries -------------------
intelmpi/4.0.0.028<span class="o">(</span>default<span class="o">)</span>  openmpi/1.4.3-intel
mkl/10.2.5.035<span class="o">(</span>default<span class="o">)</span>      otf/1.7.0<span class="o">(</span>default<span class="o">)</span>
openmpi/1.4.2<span class="o">(</span>default<span class="o">)</span>       unimci/1.0.1<span class="o">(</span>default<span class="o">)</span>
openmpi/1.4.3-gnu            vampirtrace/intel-11.1/5.8.2

--------------------- /opt/Modules/3.2.8/modulefiles/tools ---------------------
cinderclient/1.0.4<span class="o">(</span>default<span class="o">)</span>   moab/5.4.0<span class="o">(</span>default<span class="o">)</span>
cloudmesh/0.8<span class="o">(</span>default<span class="o">)</span>        myhadoop/0.2a
euca2ools/1.2                 novaclient/2.13.0<span class="o">(</span>default<span class="o">)</span>
euca2ools/1.3.1               precip/0.1<span class="o">(</span>default<span class="o">)</span>
euca2ools/2.0.2<span class="o">(</span>default<span class="o">)</span>      python/2.7<span class="o">(</span>default<span class="o">)</span>
genesisII/2.7.0               python/2.7.2
glanceclient/0.9.0<span class="o">(</span>default<span class="o">)</span>   torque/2.4.8<span class="o">(</span>default<span class="o">)</span>
keystoneclient/0.2.3<span class="o">(</span>default<span class="o">)</span> vim/7.2
marmot/2.4.0<span class="o">(</span>default<span class="o">)</span>
</pre></div>
</div>
<p><strong>Example</strong> - load a default module (in thi case cloudmesh):</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>module load cloudmesh
</pre></div>
</div>
<p>Please note that for loading the default you do not have to specify the version number.</p>
</div>
<div class="section" id="list-of-available-modules-on-various-machines">
<h3><a class="toc-backref" href="#id5">2.1.2. List of Available Modules on Various Machines</a><a class="headerlink" href="#list-of-available-modules-on-various-machines" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Module</th>
<th class="head">hotel</th>
<th class="head">india</th>
<th class="head">sierra</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>R</td>
<td>&nbsp;</td>
<td>2.11.1</td>
<td>2.11.1</td>
</tr>
<tr class="row-odd"><td>atlas</td>
<td>3.9.35</td>
<td>3.10.1</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>cbench</td>
<td>20110407-openmpi</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>cinderclient</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>1.0.4</td>
</tr>
<tr class="row-even"><td>cloudmesh</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>0.8</td>
</tr>
<tr class="row-odd"><td>cmake</td>
<td>2.8.4</td>
<td>2.8.1</td>
<td>2.8.1</td>
</tr>
<tr class="row-even"><td>ctool</td>
<td>2.12</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>euca2ools</td>
<td>&nbsp;</td>
<td>2.1.2</td>
<td>2.0.2</td>
</tr>
<tr class="row-even"><td>fftw</td>
<td>3.2.2</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>glanceclient</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>0.9.0</td>
</tr>
<tr class="row-even"><td>globus</td>
<td>5.0.3</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>goto2</td>
<td>1.13</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>gromacs</td>
<td>4.5.4</td>
<td>4.0.7</td>
<td>4.0.7</td>
</tr>
<tr class="row-odd"><td>gsl</td>
<td>1.14</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>hadoop</td>
<td>0.20.203.0</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>hdf5</td>
<td>1.8.7</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>hostlists</td>
<td>0.2</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>hpcc</td>
<td>&nbsp;</td>
<td>1.3.1</td>
<td>1.3.1</td>
</tr>
<tr class="row-even"><td>intel</td>
<td>11.1</td>
<td>11.1</td>
<td>11.1</td>
</tr>
<tr class="row-odd"><td>intelmpi</td>
<td>4.0.0.028</td>
<td>4.0.0.028</td>
<td>4.0.0.028</td>
</tr>
<tr class="row-even"><td>java</td>
<td>1.6.0_31-x86_64</td>
<td>1.6.0-x86_64</td>
<td>1.6.0-x86_64</td>
</tr>
<tr class="row-odd"><td>keystoneclient</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>0.2.3</td>
</tr>
<tr class="row-even"><td>lapack</td>
<td>3.3.0</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>marmot</td>
<td>2.4.0</td>
<td>2.4.0</td>
<td>2.4.0</td>
</tr>
<tr class="row-even"><td>mkl</td>
<td>10.2.5.035</td>
<td>10.2.5.035</td>
<td>10.2.5.035</td>
</tr>
<tr class="row-odd"><td>moab</td>
<td>&nbsp;</td>
<td>5.4.0</td>
<td>5.4.0</td>
</tr>
<tr class="row-even"><td>myhadoop</td>
<td>0.2a</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>ncbi</td>
<td>&nbsp;</td>
<td>2.2.23</td>
<td>2.2.23</td>
</tr>
<tr class="row-even"><td>novaclient</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>2.13.0</td>
</tr>
<tr class="row-odd"><td>openmpi</td>
<td>1.4.5</td>
<td>1.4.3-gnu</td>
<td>1.4.2</td>
</tr>
<tr class="row-even"><td>otf</td>
<td>1.7.1</td>
<td>1.7.0</td>
<td>1.7.0</td>
</tr>
<tr class="row-odd"><td>precip</td>
<td>&nbsp;</td>
<td>0.1</td>
<td>0.1</td>
</tr>
<tr class="row-even"><td>python</td>
<td>2.7</td>
<td>2.7</td>
<td>2.7</td>
</tr>
<tr class="row-odd"><td>szip</td>
<td>2.1</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>taktuk</td>
<td>3.7.3</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>torque</td>
<td>&nbsp;</td>
<td>2.5.5</td>
<td>2.4.8</td>
</tr>
<tr class="row-even"><td>totalview</td>
<td>&nbsp;</td>
<td>8.8.0-2</td>
<td>8.8.0-2</td>
</tr>
<tr class="row-odd"><td>unimci</td>
<td>1.0.1</td>
<td>1.0.1</td>
<td>1.0.1</td>
</tr>
<tr class="row-even"><td>vampirtrace</td>
<td>5.9</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>zookeeper</td>
<td>3.3.5</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="filesystem-layout">
<h3><a class="toc-backref" href="#id6">2.1.3. Filesystem Layout</a><a class="headerlink" href="#filesystem-layout" title="Permalink to this headline">¶</a></h3>
<dl class="docutils">
<dt><em>Home</em> directories:</dt>
<dd>Home directories are accessible through the $HOME shell variable are
located at <em>/N/u/&lt;username&gt;</em>. This is where users are encouraged to
keep source files, configuration files and executables. Users
should not run code from their $HOME directories. Please note that
this is an NFS file system, and may result in slower access for
some applications. We also advise the users to provide external
backup storage at their home institution or a code repository. For
example, we recommend that you use git or svn to make sure you
backup your changes to the code. Also make sure you backup your
data. As a testbed, we do not guarantee dataloss.</dd>
<dt><em>Scratch</em> directories:</dt>
<dd>Scratch directories are located at different locations on the
systems. To find out more about the file layout, please see the
section <a class="reference internal" href="storage.html#s-storage"><em>Storage Services</em></a></dd>
<dt><em>System software</em> directories:</dt>
<dd>System software directories are located at <em>/N/soft</em>. System and
community software are typically installed here. Table
<a class="reference internal" href="#t-storage-mountpoint"><em>Storage mountpoints on the Clusters</em></a> provides a summary of the various mount
points.</dd>
</dl>
<table border="1" class="docutils" id="t-storage-mountpoint">
<caption>Storage mountpoints on the Clusters</caption>
<colgroup>
<col width="9%" />
<col width="12%" />
<col width="4%" />
<col width="8%" />
<col width="12%" />
<col width="10%" />
<col width="43%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">    Clustername (site)</th>
<th class="head">    Mountpoint</th>
<th class="head">    Size</th>
<th class="head">    Type</th>
<th class="head">    Backups</th>
<th class="head">    Use</th>
<th class="head">    Notes</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><p class="first last">    Sierra (UCSD/SDSC)</p>
</td>
<td><p class="first last">    /N/u/portalname</p>
</td>
<td><p class="first last">    40.6TB</p>
</td>
<td><p class="first last">    ZFS  (RAID2)</p>
</td>
<td><p class="first last">    Yes  (nightly incremental)</p>
</td>
<td><p class="first last">    Home dir</p>
</td>
<td><p class="first last">    By default quotas on home directories are 50 GB and quotas on scratch directories are 100 GB.</p>
</td>
</tr>
<tr class="row-odd"><td><p class="first last">    Sierra (UCSD/SDSC)</p>
</td>
<td><p class="first last">    /N/scratch/portalname</p>
</td>
<td><p class="first last">    5.44TB</p>
</td>
<td><p class="first last">    ZFS  (RAID0)</p>
</td>
<td><p class="first last">    No</p>
</td>
<td><p class="first last">    Scratch</p>
</td>
<td><p class="first last">  </p>
</td>
</tr>
<tr class="row-even"><td><p class="first last">    Sierra (UCSD/SDSC)</p>
</td>
<td><p class="first last">    /N/soft</p>
</td>
<td><p class="first last">    50GB</p>
</td>
<td><p class="first last">    ZFS  (RAID2)</p>
</td>
<td><p class="first last">    Yes  (nightly incremental)</p>
</td>
<td><p class="first last">    Software installs</p>
</td>
<td><p class="first last">  </p>
</td>
</tr>
<tr class="row-odd"><td><p class="first last">    Sierra (UCSD/SDSC)</p>
</td>
<td><p class="first last">    /N/images</p>
</td>
<td><p class="first last">    6TB</p>
</td>
<td><p class="first last">    ZFS  (RAID2)</p>
</td>
<td><p class="first last">    Yes  (nightly incremental)</p>
</td>
<td><p class="first last">    VM images</p>
</td>
<td><p class="first last">  </p>
</td>
</tr>
<tr class="row-even"><td><p class="first last">    India  (IU)</p>
</td>
<td><p class="first last">    /N/u/portalname</p>
</td>
<td><p class="first last">    15TB</p>
</td>
<td><p class="first last">    NFS  (RAID5)</p>
</td>
<td><p class="first last">    Yes  (nightly incremental)</p>
</td>
<td><p class="first last">    Home dir</p>
</td>
<td><p class="first last">    At the moment we do not have any quota implemented on India and we use the local/tmp  (77 GB) as scratch space.</p>
</td>
</tr>
<tr class="row-odd"><td><p class="first last">    India  (IU)</p>
</td>
<td><p class="first last">    /share/project</p>
</td>
<td><p class="first last">    14TB</p>
</td>
<td><p class="first last">    NFS  (RAID5)</p>
</td>
<td><p class="first last">    Yes  (nightly incremental)</p>
</td>
<td><p class="first last">    Shared/group folders</p>
</td>
<td><p class="first last">  </p>
</td>
</tr>
<tr class="row-even"><td><p class="first last">    India  (IU)</p>
</td>
<td><p class="first last">    /tmp</p>
</td>
<td><p class="first last">    77GB</p>
</td>
<td><p class="first last">    local disk</p>
</td>
<td><p class="first last">    No</p>
</td>
<td><p class="first last">    Scratch</p>
</td>
<td><p class="first last">  </p>
</td>
</tr>
<tr class="row-odd"><td><p class="first last">    Bravo  (IU)</p>
</td>
<td><p class="first last">    /N/u/portalname</p>
</td>
<td><p class="first last">    15TB</p>
</td>
<td><p class="first last">    NFS  (RAID5)</p>
</td>
<td><p class="first last">    Yes  (nightly incremental)</p>
</td>
<td><p class="first last">    Home dir</p>
</td>
<td><p class="first last">    The same NFS shares in India are mounted in Bravo   (users do not log in here; jobs are submitted through India). There  are two local partitions which are used for HDFS and swift tests.</p>
</td>
</tr>
<tr class="row-even"><td><p class="first last">    Bravo  (IU)</p>
</td>
<td><p class="first last">    /share/project</p>
</td>
<td><p class="first last">    14TB</p>
</td>
<td><p class="first last">    NFS  (RAID5)</p>
</td>
<td><p class="first last">    Yes  (nightly incremental)</p>
</td>
<td><p class="first last">    Shared/group folders</p>
</td>
<td><p class="first last">  </p>
</td>
</tr>
<tr class="row-odd"><td><p class="first last">    Delta  (IU)</p>
</td>
<td><p class="first last">    /N/u/portalname</p>
</td>
<td><p class="first last">    15TB</p>
</td>
<td><p class="first last">    NFS  (RAID5)</p>
</td>
<td><p class="first last">    Yes  (nightly incremental)</p>
</td>
<td><p class="first last">    Home dir</p>
</td>
<td><p class="first last">    Same as Bravo. The NFS shares are mounted for user and group share (users do not log in directly here; jobs are submitted through India).</p>
</td>
</tr>
<tr class="row-even"><td><p class="first last">    Delta (IU)</p>
</td>
<td><p class="first last">    /share/project</p>
</td>
<td><p class="first last">    14TB</p>
</td>
<td><p class="first last">    NFS (RAID5)</p>
</td>
<td><p class="first last">    Yes (nightly incremental)</p>
</td>
<td><p class="first last">    Shared/group folders</p>
</td>
<td><p class="first last">  </p>
</td>
</tr>
<tr class="row-odd"><td><p class="first last">    Hotel (UC)</p>
</td>
<td><p class="first last">    /gpfs/home</p>
</td>
<td><p class="first last">    15TB</p>
</td>
<td><p class="first last">    GPFS (RAID6)</p>
</td>
<td><p class="first last">    No</p>
</td>
<td><p class="first last">    Home dir</p>
</td>
<td><p class="first last">    By default quotas on home directories are 10 GB.</p>
</td>
</tr>
<tr class="row-even"><td><p class="first last">    Hotel (UC)</p>
</td>
<td><p class="first last">    /gpfs/scratch</p>
</td>
<td><p class="first last">    57TB</p>
</td>
<td><p class="first last">    GPFS (RAID6)</p>
</td>
<td><p class="first last">    No</p>
</td>
<td><p class="first last">    Scratch</p>
</td>
<td><p class="first last">  </p>
</td>
</tr>
<tr class="row-odd"><td><p class="first last">    Hotel (UC)</p>
</td>
<td><p class="first last">    /gpfs/software</p>
</td>
<td><p class="first last">    7.1GB</p>
</td>
<td><p class="first last">    GPFS (RAID6)</p>
</td>
<td><p class="first last">    No</p>
</td>
<td><p class="first last">    Software installs</p>
</td>
<td><p class="first last">  </p>
</td>
</tr>
<tr class="row-even"><td><p class="first last">    Hotel (UC)</p>
</td>
<td><p class="first last">    /gpfs/images</p>
</td>
<td><p class="first last">    7.1TB</p>
</td>
<td><p class="first last">    GPFS (RAID6)</p>
</td>
<td><p class="first last">    No</p>
</td>
<td><p class="first last">    VM images</p>
</td>
<td><p class="first last">  </p>
</td>
</tr>
<tr class="row-odd"><td><p class="first last">    Hotel (UC)</p>
</td>
<td><p class="first last">    /scratch/local</p>
</td>
<td><p class="first last">    862GB</p>
</td>
<td><p class="first last">    ext3 (local disk)</p>
</td>
<td><p class="first last">    No</p>
</td>
<td><p class="first last">    Local scratch</p>
</td>
<td><p class="first last">  </p>
</td>
</tr>
<tr class="row-even"><td><p class="first last">    Foxtrot (UFL)</p>
</td>
<td><p class="first last">    /N/u/portalname</p>
</td>
<td><p class="first last">    16TiB</p>
</td>
<td><p class="first last">    NFS (RAID5)</p>
</td>
<td><p class="first last">    No</p>
</td>
<td><p class="first last">    Home dir</p>
</td>
<td><p class="first last">    At the moment we do not have any quota implemented on Foxtrot.</p>
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="message-passing-interface-mpi">
<h2><a class="toc-backref" href="#id7">2.2. Message Passing Interface (MPI)</a><a class="headerlink" href="#message-passing-interface-mpi" title="Permalink to this headline">¶</a></h2>
<p>The <em>Message Passing Interface Standard (MPI)</em> is the <em>de facto</em>
standard communication library for almost many HPC systems, and is
available in a variety of implementations. It has been created through
consensus of the MPI Forum, which has dozens of participating
organizations, including vendors, researchers, software library
developers, and users. The goal of the Message Passing Interface is to
provide a portable, efficient, and flexible standard for programs
using message passing. For more information about MPI, please visit:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.mpi-forum.org/">http://www.mpi-forum.org/</a></li>
<li><a class="reference external" href="http://www.mcs.anl.gov/research/projects/mpi/tutorial/">http://www.mcs.anl.gov/research/projects/mpi/tutorial/</a></li>
<li><a class="reference external" href="http://www.open-mpi.org/">http://www.open-mpi.org/</a></li>
</ul>
<div class="section" id="mpi-libraries">
<h3><a class="toc-backref" href="#id8">2.2.1. MPI Libraries</a><a class="headerlink" href="#mpi-libraries" title="Permalink to this headline">¶</a></h3>
<p>Several FutureGrid systems support MPI as part of their HPC services.
An up to date status about it can be retrieved via our <a class="reference external" href="http://inca.futuregrid.org:8080/inca/jsp/status.jsp?suiteNames=HPC,HPC_Tests,Benchmarks&amp;resourceIds=FG_BATCH">Inca
status pages</a>.</p>
<div class="admonition-todo admonition" id="index-1">
<p class="first admonition-title">Todo</p>
<p class="last">this table is outdated.</p>
</div>
<table border="1" class="docutils">
<caption>MPI versions installed on FutureGrid HPC services</caption>
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">System</th>
<th class="head">MPI version</th>
<th class="head">Compiler</th>
<th class="head">Infiniband Support</th>
<th class="head">Module</th>
<th class="head">&nbsp;</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>Alamo</td>
<td>OpenMPI 1.4.5</td>
<td>Intel 11.1</td>
<td>yes</td>
<td>openmpi</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>Bravo</td>
<td>OpenMPI 1.4.2</td>
<td>Intel 11.1</td>
<td>no</td>
<td>openmpi</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>&nbsp;</td>
<td>OpenMPI 1.4.3</td>
<td>gcc 4.4.6</td>
<td>no</td>
<td>openmpi/1.4.3-gnu</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>&nbsp;</td>
<td>OpenMPI 1.4.3</td>
<td>Intel 11.1</td>
<td>no</td>
<td>openmpi/1.4.3-intel</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>&nbsp;</td>
<td>OpenMPI 1.5.4</td>
<td>gcc 4.4.6</td>
<td>no</td>
<td>openmpi/1.5.4-[gnu</td>
<td>intel]</td>
</tr>
<tr class="row-odd"><td>Hotel</td>
<td>OpenMPI 1.4.3</td>
<td>gcc 4.1.2</td>
<td>yes</td>
<td>openmpi</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>India</td>
<td>OpenMPI 1.4.2</td>
<td>Intel 11.1</td>
<td>yes</td>
<td>openmpi</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>Sierra</td>
<td>OpenMPI 1.4.2</td>
<td>Intel 11.1</td>
<td>no</td>
<td>openmpi</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>Xray</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
<td>N/A</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
<p>Loading the OpenMPI module adds the MPI compilers to your $PATH
environment variable and the OpenMPI shared library directory to your
$LD_LIBRARY_PATH. This is an important step to ensure MPI applications
will compile and run successfully. In cases where the OpenMPI is
compiled with the Intel compilers loading the OpenMPI module will
automatically load the Intel compilers as a dependency. To load the
default openmpi module and associated compilers, just use:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>module load openmpi
</pre></div>
</div>
</div>
<div class="section" id="compiling-mpi-applications">
<h3><a class="toc-backref" href="#id9">2.2.2. Compiling MPI Applications</a><a class="headerlink" href="#compiling-mpi-applications" title="Permalink to this headline">¶</a></h3>
<p>To compile MPI applications, users can simply use the available mpi
compile commands:</p>
<dl class="docutils">
<dt>mpicc:</dt>
<dd>To compile C programs with the the CC/icc/gcc compilers</dd>
<dt>mpicxx:</dt>
<dd>To compile c++ programs with CXX/icpc/g++ with mpicxx</dd>
<dt>mpif90:</dt>
<dd>To compile programs with F90/F77/FC/ifort/gfortran</dd>
</dl>
<p>To see in detail what these commands do you can add a <em>-show</em>  as an
option. Thus the following commands:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>mpicc -show
<span class="nv">$ </span>mpicxx -show
<span class="nv">$ </span>mpif90 -show
</pre></div>
</div>
<p>will show you the detail of each of them. The resulting output can be
used as a template to adapt compile flags in case the default settings are
not suitable for you.</p>
<p>Assuming you have loaded the openmpi module into your environment,
you can compile a <a class="reference external" href="ring">simple MPI application</a> with
easily as executing:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>mpicc -o ring ring.c
</pre></div>
</div>
<p>Users MUST NOT run jobs on the login or headnodes. These nodes are
reserved for editing and compiling programs. Furthermore running your
commands on such nodes will not provide any useful information as you
actually do not use the standard cluster node.</p>
</div>
<div class="section" id="batch-jobs">
<h3><a class="toc-backref" href="#id10">2.2.3. Batch Jobs</a><a class="headerlink" href="#batch-jobs" title="Permalink to this headline">¶</a></h3>
<p>Once your MPI application is compiled, you run it on the compute nodes
of a cluster via a batch processing. With the help of a batch
processing services a job is run on the cluster without the users
intervention via a job queue. The user does not have to worry much
about the internal details of the job queue, but must provide the
scheduler with some guidance about the job so it can be efficiently
scheduled on the system.</p>
<p>To run jobs on resources with the HPC services, users must first
activate their environment to use the job scheduler:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>module load torque
</pre></div>
</div>
<p>A complete manual for the torque scheduler can be found in the <a href="#id1"><span class="problematic" id="id2">`Torque
manual&lt;http://www.clusterresources.com/torquedocs21/&gt;`__</span></a>.</p>
<p>Next we need to create a script so we can run the program on the
cluster.  We will be using our simple ring example to illustrate some
of the parameters you need to adjust:</p>
<div class="highlight-bash"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21</pre></div></td><td class="code"><div class="highlight"><pre> <span class="c">#! /bin/bash</span>

 <span class="c"># OPTIONS FOR THE SCRIPT</span>
 <span class="c">#PBS -M username@example.com</span>
 <span class="c">#PBS -N ring_test</span>
 <span class="c">#PBS -o ring_$PBS_JOBID.out</span>
 <span class="c">#PBS -e ring_$PBS_JOBID.err</span>
 <span class="c">#PBS -q short</span>
 <span class="c">#PBS -l nodes=4:ppn=8</span>
 <span class="c">#PBS -l walltime=00:20:00</span>


 <span class="c"># make sure MPI is in the environment</span>
 module load openmpi

 <span class="c"># launch the parallel application with the correct number of process</span>
 <span class="c"># Typical usage: mpirun -np &lt;number of processes&gt; &lt;executable&gt; &lt;arguments&gt;</span>
 mpirun -np 32 ring -t 1000

 <span class="nb">echo</span> <span class="s2">&quot;Nodes allocated to this job: &quot;</span>
 cat <span class="nv">$PBS_NODEFILE</span>
</pre></div>
</td></tr></table></div>
<p>In the job script, lines that begin with  <strong>#PBS</strong> are directives to
the job scheduler. You can disable any of these lines by adding an
extra  <strong>#</strong> character at the beginning of the line, as <em>##</em> is
interpreted to be a comment. Common options include:</p>
<ul class="simple">
<li>-M: specify a mail address that is notified upon completion</li>
<li>-N: To specify a job name</li>
<li>-o: The name of the file to write stdout to</li>
<li>-e: The name of the file to write stderr to</li>
<li>-q: The queue to submit the job to</li>
<li>-l: Resources specifications to execute the job</li>
</ul>
<p>The first parameters are rather obvious, so let us focus on the
<em>-q</em> option. Each batch service is configured with a number of
queues that are targeting different classes of jobs to more
efficiently schedule them. These queues can be switch on or of,
modified or new queues can be added to the system. It is useful to get
a list of available queues on the system where you like to submit your
jobs and inspect which would be most suitable to use for your
purpose with the qstat command on the appropriate login node:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>qstat -q
</pre></div>
</div>
<p>Currently we have the following queues:</p>
<dl class="docutils">
<dt>HPC Job Queue Information:</dt>
<dd><table border="1" class="first last docutils">
<colgroup>
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
<col width="20%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Resource</th>
<th class="head">Queue name</th>
<th class="head">Default Wallclock Limit</th>
<th class="head">Max Wallclock Limit</th>
<th class="head">NOTES</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>india</td>
<td>batch</td>
<td>4 hours</td>
<td>24 hours</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>&nbsp;</td>
<td>long</td>
<td>8 hours</td>
<td>168 hours</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>&nbsp;</td>
<td>scalemp</td>
<td>8 hours</td>
<td>168 hours</td>
<td>restricted access</td>
</tr>
<tr class="row-odd"><td>&nbsp;</td>
<td>b534</td>
<td>none</td>
<td>none</td>
<td>restricted access</td>
</tr>
<tr class="row-even"><td>&nbsp;</td>
<td>ajyounge</td>
<td>none</td>
<td>none</td>
<td>restricted access</td>
</tr>
<tr class="row-odd"><td>sierra</td>
<td>batch</td>
<td>4 hours</td>
<td>24 hours</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>&nbsp;</td>
<td>long</td>
<td>8 hours</td>
<td>168 hours</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>hotel</td>
<td>extended</td>
<td>none</td>
<td>none</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>alamo</td>
<td>shortq</td>
<td>none</td>
<td>24 hours</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>&nbsp;</td>
<td>longq</td>
<td>none</td>
<td>24 hours</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>foxtrot</td>
<td>batch</td>
<td>1 hour</td>
<td>none</td>
<td>not for general use</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<div class="admonition-todo admonition" id="index-2">
<p class="first admonition-title">Todo</p>
<p class="last">remove the queue ajyounge from the system, can this be done
by preserving the logs?</p>
</div>
<div class="admonition-todo admonition" id="index-3">
<p class="first admonition-title">Todo</p>
<p class="last">remove the queue b534 from the system, can this be done
while preserving the logs?</p>
</div>
<p>Next we focus on the -l option that specifies the resources. The
term:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">nodes</span><span class="o">=</span>4
</pre></div>
</div>
<p>means that we specify 4 servers on which we execute the job. The
term:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">ppn</span><span class="o">=</span>8
</pre></div>
</div>
<p>means that we use 8 virtual processors per node, while a virtual
processor is typically executed on a core of the server. Thus it is
advisable not to exceed the number of cores per server. For some
programs choosing the best performing number of servers and cores may
be dependent on factors such as memory needs, IO access and other
resource bounded properties. You may have to experiment with the
parameters. To identify the number of servers and cores available
please see Tables <a class="reference internal" href="hardware.html#t-clusters"><em>Overview of the Clusters</em></a> and <a class="reference internal" href="hardware.html#t-clusters-details"><em>Selected Details of the Clusters</em></a>.
For example, Alamo, Hotel, India, and Sierra have 8 cores per node,
thus 4 servers would provide you access to 32 processing units.</p>
<p>Often you may just want to have the stdout and stderr in one file,
than you simply can replace the line with -e in it with:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="c">#PBS -j oe</span>
</pre></div>
</div>
<p>which simply means that you <em>join</em> stdout and stderr. Here j stands
for join, o for stdout and e for stderr. In case you like to have
e-mail send to you based on the status of the job, you can achieve
this with adding:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="c">#PBS -m ae</span>
</pre></div>
</div>
<p>to your script. It will send you mail when the job aborts (indicated
by a), or when the job ends (indicated by e).</p>
</div>
</div>
<div class="section" id="job-management">
<h2><a class="toc-backref" href="#id11">2.3. Job Management</a><a class="headerlink" href="#job-management" title="Permalink to this headline">¶</a></h2>
<p>A list of all available  scheduler commands is available from the <a class="reference external" href="http://www.clusterresources.com/torquedocs21/">Torque
manual page</a>. We
describe next the use of some typical interactions to manage your jobs
in the batch queue.</p>
<div class="section" id="job-submission">
<h3><a class="toc-backref" href="#id12">2.3.1. Job Submission</a><a class="headerlink" href="#job-submission" title="Permalink to this headline">¶</a></h3>
<p>Once you have created a submission script, you can then use the
qsub command to submit this job to be executed on the compute nodes:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>qsub ring.pbs
20311.i136
</pre></div>
</div>
<p>The qsub command outputs either a job identifier or an error message
describing why the scheduler would not accept your job. Alternatively,
you can also use the msub command, which is very similar to the qsub
command. For differences we ask you to consult the man pages.</p>
</div>
<div class="section" id="job-deletion">
<h3><a class="toc-backref" href="#id13">2.3.2. Job Deletion</a><a class="headerlink" href="#job-deletion" title="Permalink to this headline">¶</a></h3>
<p>Sometimes you may want to delete a job from the queue, which can be
easily done with the qdel command, followed by the id:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>qdel 20311
</pre></div>
</div>
</div>
<div class="section" id="job-monitoring">
<h3><a class="toc-backref" href="#id14">2.3.3. Job Monitoring</a><a class="headerlink" href="#job-monitoring" title="Permalink to this headline">¶</a></h3>
<p>If your job is submitted successfully, you can track its execution
using the qstat or showq commands. Both commands will show you the
state of the jobs submitted to the scheduler. The difference is mostly
in their output format.</p>
<dl class="docutils">
<dt>showq:</dt>
<dd><p class="first">divides the output into three sections:  active
jobs,  eligible jobs, and blocked jobs:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>showq
active <span class="nb">jobs</span>
------------------------
JOBID    USERNAME       STATE PROCS    REMAINING            STARTTIME
20311   yourusername       Running     16        3:59:59 Tue Aug 17 09:02:40
1 active job 16 of 264 processors in use by <span class="nb">local jobs</span> <span class="o">(</span>6.06%<span class="o">)</span>
                  2 of 33 nodes active <span class="o">(</span>6.06%<span class="o">)</span> eligible <span class="nb">jobs</span>
----------------------
JOBID    USERNAME       STATE PROCS    REMAINING            STARTTIME
0 eligible <span class="nb">jobs </span>blocked <span class="nb">jobs</span>
-----------------------
JOBID    USERNAME       STATE PROCS    REMAINING            STARTTIME
0 blocked <span class="nb">jobs</span>
Total job: 1
</pre></div>
</div>
<dl class="last docutils">
<dt>Legend:</dt>
<dd><dl class="first last docutils">
<dt>Active jobs:</dt>
<dd>are jobs that are currently running on resources.</dd>
<dt>Eligible jobs:</dt>
<dd>are jobs that are waiting for nodes to become available before
they can run. As a general rule, jobs are listed in the order
that they will be scheduled, but scheduling algorithms may
change the order over time.</dd>
<dt>Blocked jobs:</dt>
<dd>are jobs that the scheduler cannot run for some reason. Usually
a job becomes blocked because it is requesting something that
is impossible, such as more nodes than currently exist, or more
processors per node than are installed.</dd>
</dl>
</dd>
</dl>
</dd>
<dt>qstat:</dt>
<dd><p class="first">provides a single table view, where the status of each job is
added via a status column called S:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>qstat
Job id                             Name               User          Time Use S Queue
------------------------- --------------------- ------------------- -------- - -----
1981.i136                       sub19327.sub      inca               00:00:00 C batch
20311.i136                      testjob           yourusername              0 R batch
</pre></div>
</div>
<dl class="last docutils">
<dt>Legend:</dt>
<dd><dl class="first last docutils">
<dt>Job id:</dt>
<dd>is the identifier assigned to your job.</dd>
<dt>Name:</dt>
<dd>is the name that you assigned to your job.</dd>
<dt>User:</dt>
<dd>is the username of the person who submitted the job.</dd>
<dt>Time:</dt>
<dd>is the amount of time the job has been running.</dd>
<dt>S:</dt>
<dd>shows the job state. Common job states are R for a running job, Q
for a job that is queued and waiting to run, C for a job that has
completed, and H for a job that is being held.</dd>
<dt>Queue:</dt>
<dd>is the name of the job queue where your job will run.</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<p>If you are interested in only your job by for example using grep:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>qstat | grep 20311
</pre></div>
</div>
</div>
<div class="section" id="job-output">
<h3><a class="toc-backref" href="#id15">2.3.4. Job Output</a><a class="headerlink" href="#job-output" title="Permalink to this headline">¶</a></h3>
<p>If you gave your job a name with the  <strong>#PBS -N &lt;jobname&gt;</strong> directive
in your job script or by specifying the job name on the command line,
your job output will be available in a file named  <strong>jobname.o######</strong>,
where the  <strong>######</strong> is the job number assigned by the job manager.
You can type  <strong>ls jobname.o*</strong> to see all output files from the same
job name.</p>
<p>If you explicitly name an output file with the  <strong>#PBS -o
&lt;outfile&gt;</strong> directive in your job script or by specifying the output
file on the command line, your output will be in the file you specified.
If you run the job again, the output file will be overwritten.</p>
<p>If you don&#8217;t specify any output file, your job output will have the same
name as your job script, and will be numbered in the same manner as if
you had specified a job name (<strong>jobname,o######</strong>).</p>
</div>
</div>
<div class="section" id="xray-hpc-services">
<h2><a class="toc-backref" href="#id16">2.4. Xray HPC Services</a><a class="headerlink" href="#xray-hpc-services" title="Permalink to this headline">¶</a></h2>
<p>To log into the login node of xreay please use the command:</p>
<div class="highlight-bash"><div class="highlight"><pre>ssh portalname@xray.futuregrid.org
</pre></div>
</div>
<p>Extensive documentation about the user environment of the Cray can be
found at</p>
<ul class="simple">
<li><a class="reference external" href="http://docs.cray.com/cgi-bin/craydoc.cgi?mode=View;id=S-2396-21">Cray XTTM Programming Environment User&#8217;s Guide</a></li>
</ul>
<p>For MPI jobs, use cc (pgcc). For best performance, add the xtpe-barcelona module:</p>
<div class="highlight-bash"><div class="highlight"><pre>% module add xtpe-module
</pre></div>
</div>
<p>Currently there is only one queue (batch) available to users on the
Cray, and all jobs are automatically routed to that queue.  You can
use the same commands as introduced in the previous sections. Thus, to
list the queues please use:</p>
<div class="highlight-bash"><div class="highlight"><pre>qstat -Q
</pre></div>
</div>
<p>To obtain details of running jobs and available processors, use the showq command:</p>
<div class="highlight-bash"><div class="highlight"><pre>/opt/moab/default/bin/showq
</pre></div>
</div>
<div class="section" id="submitting-a-job-on-xray">
<h3><a class="toc-backref" href="#id17">2.4.1. Submitting a Job on xray</a><a class="headerlink" href="#submitting-a-job-on-xray" title="Permalink to this headline">¶</a></h3>
<p>To execute an MPI program on xray we use a special program called aprun in
the submit script. Additionally we have some special resource
specifications that we can pass along, such as mppwidth and
mppppn. An example is the following program that will use 16
processors on 2 nodes:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>cat job.pbs
</pre></div>
</div>
<div class="highlight-bash"><div class="highlight"><pre><span class="c">#! /bin/sh</span>

<span class="c">#PBS -l mppwidth=16</span>
<span class="c">#PBS -l mppnppn=8</span>
<span class="c">#PBS -N hpcc-16</span>
<span class="c">#PBS -j oe</span>
<span class="c">#PBS -l walltime=7:00:00</span>

<span class="c">#cd to directory where job was submitted from</span>
<span class="nb">cd</span> <span class="nv">$PBS_O_WORKDIR</span>
<span class="nb">export </span><span class="nv">MPICH_FAST_MEMCPY</span><span class="o">=</span>1
<span class="nb">export </span><span class="nv">MPICH_PTL_MATCH_OFF</span><span class="o">=</span>1
aprun -n 16 -N 8 -ss -cc cpu hpcc

<span class="nv">$ </span>qsub job.pbs
</pre></div>
</div>
<p>The XT5m is a 2D mesh of nodes. Each node has two sockets, and each
socket has four cores. The batch scheduler interfaces with a Cray
resource scheduler called APLS. When you submit a job, the batch
scheduler talks to ALPS to find out what resources are available, and
ALPS then makes the reservation.</p>
<p>Currently ALPS is a &#8220;gang scheduler&#8221; and only allows one &#8220;job&#8221; per node.
If a user submits a job in the format aprun -n 1 a.out , ALPS will put
that job on one core of one node and leave the other seven cores empty.
When the next job comes in, either from the same user or a different
one, it will schedule that job to the next node.</p>
<p>If the user submits a job with aprun -n 10 a.out , then the scheduler
will put the first eight tasks on the first node and the next two tasks
on the second node, again leaving six empty cores on the second node.
The user can modify the placement with -N , -S , and -cc .</p>
<p>A user might also run a single job with multiple treads, as with OpenMP.
If a user runs this job aprun -n 1 -d 8 a.out , the job will be
scheduled to one node and have eight threads running, one on each core.</p>
<p>You can run multiple, different binaries at the same time on the same
node, but only from one submission. Submitting a script like this
will not work:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>1 aprun -n 1 -d 1 -cc 0 ./my-binary
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>1 aprun -n 1 -d 1 -cc 1 ./my-binary
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>1 aprun -n 1 -d 1 -cc 2 ./my-binary
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>1 aprun -n 1 -d 1 -cc 3 ./my-binary
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>1 aprun -n 1 -d 1 -cc 4 ./my-binary
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>1 aprun -n 1 -d 1 -cc 5 ./my-binary
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>1 aprun -n 1 -d 1 -cc 6 ./my-binary
<span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>1 aprun -n 1 -d 1 -cc 7 ./my-binary
</pre></div>
</div>
<p>This will run a job on each core, but not at the same time. To run all
jobs at the same time, you need to first add all the binaries within
one aprun command:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ </span>cat run-all.pbs
./my-binary1
./my-binary2
./my-binary3
./my-binary4
./my-binary5
./my-binary6
./my-binary7
./my-binary8
<span class="nv">$ </span>aprun -n 1 run.pbs
</pre></div>
</div>
<p>Alternatively, use the command aprun -n 1 -d 8 run.pbs. To run multiple
serial jobs, you must build a batch script to divide the number of jobs
into groups of eight, and the</p>
</div>
</div>
</div>


</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
    </p>
  </div>
</footer>
  </body>
</html>