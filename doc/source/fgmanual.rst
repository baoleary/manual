

Getting Started
===============

In order for you to gain access to the FutureGrid resources, you need to
follow real simple steps:

#. Create a FutureGrid portal account
#. Create or join a project
#. Upload your SSH key
#. Explore our Manual

We will introduce these steps in more detail next.

Step 1: Create a FutureGrid Portal Account
------------------------------------------

In order to utilize \ **any** FutureGrid resource, you must posses a
FutureGrid \ **portal account**. Thus, \ *apply for your \ **portal
account ***\ before you attempt anything else. This account is used to
gather some information that we will use in the next steps. You must
make sure that the information is complete before you proceed to the
second step.  FutureGrid performs basic verification of the information
you provide when creating an account, so it may take a little while
before your account is approved. Once you have a portal account, please
proceed.

Please note that you cannot access FutureGrid resources until you
complete the next steps.  

Here are a few tips that make it easy for you

-  On the portal's main page at
   `https://portal.futuregrid.org <https://portal.futuregrid.org>`__
   appears
   a \ `**Register** <https://portal.futuregrid.org/user/register%20>`__ Link. 
-  Following you will be able to \ **Create a new account** on the
   portal. 
-  Fill in **ALL **\ fields as much as you can.
-  Note that fields with \* are mandatory
-  It is important that you specify your address information completely.
-  If you are a graduate or undergraduate student please fill out your
   advisors contact information in the field specially dedicated for it.
   If he has a FutureGrid Portal name, please also add his portal name
   if you know it in that field.
-  If you have an e-mail address from your institution, we ask that you
   use this address instead of one from gmail, hotmail, or other e-mail
   services that we cannot trace back to your name or institution.
-  Usage of all non institutional addresses will prolong the application
   process.
     
-  Please note that creating a portal account does not give you access
   to any FutureGrid resources, for that you have to complete step 2 and
   3.
     
-  Please remember that checking your information will take time. Thus
   we recommend that you wait till you get a message that tells you that
   your portal account has been approved. Then continue to The next
   step. We are not conducting any portal approval outside of 10am-4pm
   EST. If you are easily to be identified your approval will take 1-2
   days, if not, we have either problems verifying your data or
   something else is not right. In case you appear to be a spammer we
   will not notify you. 
     
-  In case you are teaching a class class we have some special
   instructions for you and after you apply for a portal account you may
   want to get in contact with us via the `help
   form <https://portal.futuregrid.org/help>`__

Step 2: Create a new Project or join an existing one
----------------------------------------------------

You need to either apply for a new FutureGrid project or join an
existing project to use FutureGrid resources. To apply for a new
project, fill out the `project creation
form <https://portal.futuregrid.org/node/add/fg-projects>`__. To join an
existing project, ask the project lead or project manager for that
project to add you to their project using that same form. If the project
is set to "accept public join request", you may also send a request in
the portal. To do this, first view the `project
list <https://portal.futuregrid.org/projects>`__\ and go to the project
detail page by clicking the project title. If the project is set by the
project lead to "accept join request", then you'll see a large gray
'Join this project' button in the upper right corner of the page. Click
the button to send the join request to the project lead and manager so
they can process your request.

Once you have been approved to work on a project, you will be able to
access the resources and services that your project has requested and
been authorized to use. See the `project creation
form <https://portal.futuregrid.org/node/add/fg-projects>`__ for a list
of FutureGrid resources and services.

Here are a view links that may help you:

-  `Create a new project creation
   form <https://portal.futuregrid.org/node/add/fg-projects>`__
     
-  `See the project list to identify the project you like to
   join <https://portal.futuregrid.org/projects>`__
     
-  `Read some FAQs about the account
   creation <https://portal.futuregrid.org/faq>`__

Step 3: Upload Your SSH Public Key(s)
-------------------------------------

In order to be able to log into the started VMs, among other purposes,
you need to provide FG with a secure-shell (ssh) public key. If you are
already a frequent user of ssh, and have a private and public key pair,
it is perfectly reasonable to provide your public key. It's \ *public*,
after all.

To upload the chosen public key:

#. Copy your public identity into your system clipboard.
#. Log into the FG portal \ `https://portal.futuregrid.org/ <../../>`__
#. In the \ **Accounts** menu, select \ **My Portal and HPC
   Account** page. and activate the panel saying **SSH keys**
#. Click the link that says Add a public key.

This step should be fairly instantaneous.

If you are not familiar with ssh key generation, or if you have
difficulty generating a key pair, please inform yourself about ssh keys
or contact us via our `help form <https://portal.futuregrid.org/help>`__
. Detailed instructions on how to generate ssh key pairs will be added
to this document in the near future.

Step 4: Explore the Documentation
  
---------------------------------

Once you have access to FutureGrid resources, a good place to start
learning about how to use FutureGrid are the tutorials, specifically the
following:

-  The \ `Manual <http://portal.futuregrid.org/manual>`__ for detailed
   information

Our manual will include a variety of topics that are of interest to our
users from many different user communities. It would be a disservice to
you to just list a view of them. Hence we provide you with a convenient
link to our table of contents. Please note that the manual also contains
links to our tutorials and our MOOC.

-  `Manual Table of
   Contents <https://portal.futuregrid.org/manual/toc>`__

It will be also easy for you to search for some topics in our search box
at the top of the Web page.


Accessing FutureGrid
====================

To use FutureGrid you must be part of a valid "project". Project
leaders are requested to fill out project applications about the use of
FutureGrid. The
`form <https://portal.futuregrid.org/node/add/fg-projects>`__ gathers
some important information about their projects to be conducted. At this
time this information is publicly shared. This information is used to
report and document not only to us but also to our sponsors which
activities are conducted on FutureGrid. The more precise you are in your
descriptions and filling out the forms the better we can highlight your
project. Once a project is formed, project members can join a project.
This must be conducted by the project lead.  A user retains an active
account on FutureGrid when they are in at least one active project. A
user that is inactive does not have to apply for a new account, but
instead apply for a new project. Once that project is activated the user
account becomes active.

Account Management Service
--------------------------

Please note the current process of applying for account may change. 

#. Any user can apply easily for a Portal account: Please go to 

   -  `https://portal.futuregrid.org/user/register <http://portal.futuregrid.org/user/register>`__
   -  it may take a day or two to get a portal account. Portal
      accounts will not be created over the weekend.

#. Once logged in the user has a couple of options

   -  `User Profile
      Management <https://portal.futuregrid.org/manage-my-portal-account>`__:
      Update information regarding the user profile
   -  Project Management: apply for `new
      projects <https://portal.futuregrid.org/node/add/fg-projects>`__,
      join existing projects, update information and results of a
      project, manage members and roles of users participating in your
      projects
   -  Managing Certificates and Keys: Integrate OpenID login for the
      portal (with for example your google ID), manage your ssh key for
      access to the HPC service, Manage Nimbus and Eucalyptus accounts
      and keys 

Apply for a Project or Request an Account
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To get access to FutureGrid (FG), you need to register your project in
it. Please go to:

-  `https://portal.futuregrid.org/node/add/fg-projects <https://portal.futuregrid.org/node/add/fg-projects>`__

to apply for a new project. It may take some time for your project to
get approved. No projects will be approved over the weekend.

In case you like to join an existing project, please find the list of
projects at:

-  `https://portal.futuregrid.org/projects <https://portal.futuregrid.org/projects>`__

Clicking on a project title will bring you to the project information
page, where you may see a large gray button on the upper right corner of
the page named 'Join this project'. Click to send join request to that
project. Please notice the PI of the project has to approve you before
you are part of that project.

Some project may not accept external members so there would be no join
button for them. Please notice this behavior is controlled by the owner
of the project.

You can always communicate with the project PI through external channel
of the FutureGrid portal if you know him/her in person by letting
him/her know your portal username. He/she can then directly add you as a
member to his/her project without going through the join request process
through the portal.

Implicit Project Responsibilities for Project Members and PI Agreement for Reporting
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The Project PI has agreed to certain reporting requirements to provide
information to FutureGrid. He will be responsible to make sure that they
are completed and also implemented with the users joining the project.
Thus the user is responsible to comply with the terms of the project in
regards to reporting and acknowledgements in case of publications. Each
project PI has the responsibility to communicate such requirements to
the members and managers. The project agreements overwrite the
individuals agreement. 

Cloud Accounts
~~~~~~~~~~~~~~

The cloud accounts are handled currently separately from the HPC account
creation process. In fact the Eucalyptus clouds are disjunctive on each
machine and have their own user management. For Nimbus uploading your
ssh key is sufficient. The turnaround time for you getting access to the
system is typically between 30 minutes and one day.

Please be kind and only apply for these cloud accounts if you really
need them.

Key Reset or Adding new Keys
----------------------------

To reset or add a new you ssh key, please update your keys first by
visiting the page

-  `https://portal.futuregrid.org/user/23/ssh-keys <https://portal.futuregrid.org/user/23/ssh-keys>`__

and change the keys as needed. Your reset will take 30 minutes to one
day to propagate through the system services.

You are not allowed to use password less keys.  Your account may be
deactivated.

 

Account Creation
================

Creating an HPC account
-----------------------

All you need to do to obtain an HPC account is to be in a valid project
and upload your ssh key. Typically you will get the account within one
business day. There will be typically no accounts approved in non
business hours including weekends.

Please add your SSH keys in your profile which you can find at:

-  `https://portal.futuregrid.org/user/23/ssh-keys <https://portal.futuregrid.org/user/23/ssh-keys>`__

Once you have done that, please go back to this form and complete it.
After submission, this form will create an e-mail request to FutureGrid.
The FG administrators may contact you to verify this request.

Please, make sure you are a member of a valid project.
 Project IDs can be found at

-  `https://portal.futuregrid.org/projects <https://portal.futuregrid.org/projects>`__

The project lead is responsible for determining if you can be added to
the project.

Note: Do not send mail to FG staff members about joining a project, as
we are not managing individual project memberships

To view your current memberships and status of your account application,
please visit

-  `https://portal.futuregrid.org/manage-my-portal-account <https://portal.futuregrid.org/manage-my-portal-account>`__

Resetting a ssh-key
-------------------

Simply visit the ssh-key page and upload a new key. YoU can optionally
delete other keys if you do n longer need them.

Nimbus, Eucalyptus, OpenStack
-----------------------------

Once you have uploaded your SSH key, the Nimbus, Eucalyptus, and
OpenStack access will be granted. Please visit the manual pages for more
details on accessing them.



Training and Education
----------------------

FutureGrid provides training and educational materials through manuals
and tutorials.

FutureGrid leverages technologies contributed by its partners and by the
open-source community in the packaging, configuration, and deployment of
virtual clusters - including the plug-and-play, self-configuring Grid
appliance, the Nimbus science cloud middleware, the IPOP/GroupVPN
self-configuring virtual network, and the ViNe virtual network.

For links to the initial appliance-based FutureGrid tutorials, see the
FutureGrid \ `Education and Outreach
page <https://portal.futuregrid.org/outreach>`__. Additionally, for
video tutorials on the use of the appliances, see the \ `Grid appliance
YouTube
channel <http://www.youtube.com/acisp2p#p/c/D77781CEF51F72F3>`__.

 

`‹ Quickstart <https://portal.futuregrid.org/gettingstarted>`__

Guide to Using the FutureGrid Portal
====================================

Functions of the FutureGrid Portal
----------------------------------

| 
|  The FutureGrid portal aims to:

#. Be the definite source for information about FutureGrid (manuals,
   papers, forums, FAQ, ...)
#. Allow management of your FG accounts (portal, services, and
   resources)
#. Allow management of your futuregrid projects
#. Allow management of FG experiments
#. Allow the dynamic provisioning via RAIN 

A FutureGrid User Dashboard 
----------------------------

After login, you will be redirected to a dashboard-like page (go to the
menu Accounts -> My Portal Account), where you will see the following:

#. A list of useful links, including links to profile, account, SSH key,
   and OpenID management information.
#. A projects summary section that lists the summarized information
   about projects that you are the owner of, that you manage, that you
   are member of, and that you support as a FutureGrid expert,
   respectively. Clicking the project title will bring you to the
   project detail info page. For those projects that you own or manage,
   an 'edit' link is also there so you can quickly update the project
   information.
#. A 'My Content' section where the content that you are responsible for
   maintaining (and/or that you have contributed) is listed. This gives
   a convenient view so you can easily go back to the content and update
   it.
#. A 'My Publications' section that lists your publications.

Update Project Information and Add Results
------------------------------------------

Another frequently used feature is the ability to update your project
information (e.g., add project members) and fill in results in the
'Project Results' section. You can do so only when you are either the
owner of the project or the project manager (if the owner has delegated
that to you). By following the links provided in the 'Dashboard'
section, you can review and edit the project information.

To add a user to your project as a member, the user must have a
FutureGrid portal account first. Then, while editing the project, you
can type a user's first name, and the suggestion feature will pop up
with the user's username to be added. If you have many members to add,
click the 'Add another item' in the 'Project Members' section to add
more. See also `this
FAQ <https://portal.futuregrid.org/how-can-i-add-people-project>`__.

For updating your project results, there is a 'Project Results' section
with a 'Results' window that supports WYSIWYG editing. It supports
simple formatted text, embedded images, etc. For text, you can edit
directly in the window, or copy the content you developed in your
favorite editor and paste the content in the edit window. In the case of
images, you'll need to upload the image to the server first, and then
insert it to the window, or alternatively refer to an external URL for
an image hosted somewhere else.

For more detailed info on how to include an image, please see `this
FAQ <https://portal.futuregrid.org/how-upload-andor-include-image-while-creating-pagenews-etc>`__.

Contribute to the FutureGrid Community
--------------------------------------

The FutureGrid portal also provides its users a place where they can
contribute to the community by sharing their ideas, research topics, FG
experience, etc.; in this way people can learn from you, and also you
can learn from others. Emphasizing user participation and
collaboration is one of the main goals shaping the portal to its current
state.

You can contribute by `creating a 'Community
Page' <https://portal.futuregrid.org/node/add/page-community>`__. You
can find the link in the left side navigation block, under the 'Create
content' menu. After entering the edit page, you'll see a 'Title' text
box, where you put the content/article title, and a 'Body' window where
you put the content. Once again, it supports formatted text and embedded
images, etc.

You can cite FutureGrid references also, by enclosing a citekey within
the 'bib' tag as stated under the editor window (NOTE: Please use '[]'
instead of '<>'), where CITEKEY could be found in the `biblio
page <https://portal.futuregrid.org/biblio>`__ (the content within but
not including the '[ ]').

An example of a user contributed page can be found
`here <https://portal.futuregrid.org/contrib/testexample-page-user-contributed-page>`__.

File Upload and Attachment to a Page
------------------------------------

Please see `this
FAQ <https://portal.futuregrid.org/faq/how-uploadattach-file-page>`__
for instructions on file upload.



Accessing FutureGrid resources via SSH
======================================

To properly view this manual page, please log into the FutureGrid portal
with your FutureGrid name.

To access the various FutureGrid resources, you need to provide a public
ssh key to FutureGrid. In this manual, we explain how to generate a ssh
key, upload it to the FutureGrid portal and log onto the resources. This
manual covers both UNIX and Windows Users.

Requirement for Windows Users
=============================

Windows users need to have some special software to be able to use the
SSH commands. We recommend you use Cygwin (Linux-like environment for
Windows) because it will ease your experience with FutureGrid. We have
prepared a Cygwin version that is ready to use (If for some reason you
decide to download and install Cygwin from the official site, remember
that you need to install the ssh packages).

#. Download Cygwin from our
   Portal \ `https://portal.futuregrid.org/sites/default/files/cygwin.zip <https://portal.futuregrid.org/sites/default/files/cygwin.zip>`__.
#. Uncompress the file.
#. Execute the file the 'Windows Batch File' called Cygwin.bat
   |image21|
     
#. You may get a warning. Click in the Run button
   |image22|
     
#. You get a Linux-like terminal that will allow you to continue with
   this manual.
   |image23|
     

**NOTE**: When showing examples of commands, the $ symbol precedes the
actual command. So, the other lines are the output obtained after
executing the command.

Instructions for both Windows and Unix users
============================================

Generate SSH key
----------------

Use the tool ssh-keygen. This program is commonly available on most UNIX
systems (this includes Cygwin). It will ask you for the location and
name of the new key. It will also ask you for a passphrase, we
**STRONGLY RECOMMEND** that you use a passphrase. We have seem advise by
teachers and teachin assistants to not use passphrases: this is
**WRONG**. If you are not using a pasphrase and someone were to steal
your private key they have easily access to your account. We recommend
using the default location ~/.ssh/ and the default name id\_rsa. A
sample session:

::

    $ ssh-keygen

    Generating public/private rsa key pair.
    Enter file in which to save the key (/home/Javi/.ssh/id_rsa): 
    Enter passphrase (empty for no passphrase):
    Enter same passphrase again:
    Your identification has been saved in /home/Javi/.ssh/id_rsa
    Your public key has been saved in /home/Javi/.ssh/id_rsa.pub.
    The key fingerprint is:
    90:46:9b:cf:09:16:94:17:df:43:f4:99:97:0d:42:4a Javi@Javi-PC
    The key's randomart image is:

::

    +--[ RSA 2048]----+
    |     .+...Eo= .       |
    |     ..=.o + o +o    |
    |      O.  o o +.o      |
    |     o = .   . .       |
    |        S                     |
    |                                 |
    |                                 |
    |                 |
    |                                 |
    +-----------------+

This command requires the interaction of the user.

    1. The first question is:

::

    Enter file in which to save the key (/home/Javi/.ssh/id_rsa): 

We recommend you use the default. To do so, just press the enter key. In
case you already have a ssh key in your machine, you can skip this whole
section or use a different file name.

    2. The second and third question is to protect your ssh key with a
passphrase. This password will protect your key because you need to type
it when you want to use it. Thus, you can either type a passphrase or
press enter to leave it without passphrase. To avoid security problems,
we DO recommend that chose a passphrase as discussed previously. Make
sure to not just type return for an empty passphrase.

::

    Enter passphrase (empty for no passphrase):

and

::

    Enter same passphrase again:

 

Check your ssh key
------------------

Once, you have generated your key, you should have them in the .ssh
directory

::

    $ ls -l ~/.ssh

Copy the content of your public key
-----------------------------------

You need to copy the content of your public key to upload it to the
portal. A sample asumming that you used the default options during the
key generation:

::

    $ cat ~/.ssh/id_rsa.pub

    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCXJH2iG2FMHqC6T/U7uB8kt6KlRh4kUOjgw9sc4Uu+Uwe/EwD0wk6CBQMB+HKb9upvCRW/851UyRUagtlQexCRM2rMCi0VvhTVZhj61pTdhyl1t8hlkoL19JVnVBPP5kIN3wVyNAJjYBrAUNW4dXKXtmfkXp98T3OW4mxAtTH434MaT+QcPTcxims/hwsUeDAVKZY7UgZhEbiExxkejtnRBHTipi0W03W05TOUGRW7EuKf/4ftNVPilCO4DpfY44NFG1xPwHeimUk+t9h48pBQj16FrUCp0rS02Pj+4/9dNeS1kmNJu5ZYS8HVRhvuoTXuAY/UVcynEPUegkp+qYnR Javi@Javi-PC

Go ahead and select the ouptut, right click, and copy

::

    $ cat ~/.ssh/id_rsa.pub

    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCXJH2iG2FMHqC6T/U7uB8kt6KlRh4kUOjgw9sc4Uu+Uwe/EwD0wk6CBQMB+HKb9upvCRW/851UyRUagtlQexCRM2rMCi0VvhTVZhj61pTdhyl1t8hlkoL19JVnVBPP5kIN3wVyNAJjYBrAUNW4dXKXtmfkXp98T3OW4mxAtTH434MaT+QcPTcxims/hwsUeDAVKZY7UgZhEbiExxkejtnRBHTipi0W03W05TOUGRW7EuKf/4ftNVPilCO4DpfY44NFG1xPwHeimUk+t9h48pBQj16FrUCp0rS02Pj+4/9dNeS1kmNJu5ZYS8HVRhvuoTXuAY/UVcynEPUegkp+qYnR Javi@Javi-PC

Upload the key to the FutureGrid Portal
---------------------------------------

Click on the button bellow to add your SSH key (Note: The quick link
button below only works if you are logged in before visiting this page.
Otherwise please make sure you login first and REFRESH this page.)
 |image24|

#. If you were logged into the Portal, this button redirects you to a
   page that includes a link Add a public key.
#. Otherwise, this button redirects you to the login page.

   #. Log into the portal
      |image25|
        
   #. Click in the "ssh key" button
      |image26|
        

#. Click in the "add a public key" link.
   |image27|
     
#. Paste your ssh key into the box marked Key.
   |image28|
     
#. Click the submit button.

-  **IMPORTANT**:

   -  Leave the Title field blank.
   -  Make sure that when you paste your key, it does not contain
      newlines or carriage returns that may have been introduced by
      incorrect pasting and copying. If so, please remove them.

At this point you are all set. However you will still need to wait till
all accounts have been set up to use the resources. Please, check your
email for further updates. You can also refresh this page and see if the
boxes in your account status information are all green. Than you can
continue.

Testing your ssh key
--------------------

Test you key by logging onto India. India cluster gets the new ssh key
updated almost immediately. For other clusters like Hotel, it can take
around 10 minutes to update the ssh keys. If you are viewing this page
anonymously, please replace <USER> with your FutureGrid user name (the
one used to log into the Portal).

If you placed the ssh key in the default location:

::

    $ ssh -A gvonlasz@india.futuregrid.org

If you used a different path or name for your key:

::

    $ ssh -A -i <path to private key> gvonlasz@india.futuregrid.org 

The first time you ssh into a machine you will see a message like the
one shown in the picture. You have to type yes and press enter:

::

**Note**: the presence of the -A argument above is required for Nimbus
tutorials.

**Note 1**: If you are asked for a password when trying to ssh onto
Hotel, do **NOT** type any password. This means that your ssh key is not
updated yet. You need to wait a bit more.

Testing your ssh key on Hotel
-----------------------------

After uploading your ssh key, it can take around 10 minutes to update
the ssh keys of Hotel. So, if you were able to log onto India, you have
set up properly your ssh key. So, after a while you will be able to log
onto Hotel.  If you are viewing this page anonymously, Please replace
<USER> with your FutureGrid user name (the one used to log into the
Portal).

If you placed the ssh key in the default location:

::

    $ ssh -A gvonlasz@hotel.futuregrid.org

If you used a different path or name for your key:

::

    $ ssh -A -i <path to private key> gvonlasz@hotel.futuregrid.org 

The first time you ssh into a machine you will see a message like this:

::

    The authenticity of host 'hotel.futuregrid.org (149.165.148.5)' can't be established.
    RSA key fingerprint is f8:96:15:b7:21:eb:64:92:6c:de:e0:79:f3:fb:86:dd.
    Are you sure you want to continue connecting (yes/no)? yes 

**Note**: the presence of the -A argument above is required for Nimbus
tutorials.

**Note 1**: If you are asked for a password when trying to ssh onto
Hotel, do \ **NOT** type any password. This means that your ssh key is
not updated yet. You need to wait a bit more.

 

HPC Services
============

Using HPC Services on FutureGrid
================================

Accessing Systems
-----------------

Several of the clusters that are part of FutureGrid have partitions
that operate as High Performance Computing (HPC) systems. These
partitions are batch scheduled, are not virtualized, have computer nodes
with fixed operating systems, and are suitable for running parallel
applications. FutureGrid provides a `list of HPC
partitions <http://inca.futuregrid.org:8080/inca/jsp/partitionTable.jsp>`__
that currently consists of nodes on Alamo, Hotel, India, Sierra, and
Xray.

To access the FutureGrid HPC partitions, you need a FutureGrid
account and an SSH public key you have uploaded to FutureGrid (this
process is described on our `Getting Started
page <https://portal.futuregrid.org/gettingstarted>`__). You can then
simply ssh to the login node of the FutureGrid system you would like to
use. These login nodes are named *<system>.futuregrid.org*,
specifically:

-  alamo.futuregrid.org
-  bravo.futuregrid.org
-  hotel.futuregrid.org
-  india.futuregrid.org
-  sierra.futuregrid.org
-  xray.futuregrid.org

If your FutureGrid username is different from your username on your
system, you will need to include it in your ssh command: *ssh
<futuregrid user name>@<system>.futuregrid.org*. You can find out your
HPC account name by visiting your Portal account page.


Filesystem Layout
-----------------

-  *Home* ($HOME) directories are located at */N/u/<username>*, with
   automated nightly backups. This is where users are encouraged to keep
   source files, configuration files and executables. Users should not
   run code from their $HOME directories. Please note that this is an
   NFS file system, and may result in slower access for some
   applications.
     
-  *Scratch* directories are located at different locations on the
   systems. To find out more about the file layout, please see `Storage
   information for FutureGrid
   hardware. <http://portal.futuregrid.org/kb/document/bcgv>`__
     
-  *System software* directories are located at\ */N/soft,* with
   automated nightly backups. System and community software are
   typically installed here.

Modules
-------

Resources in the FutureGrid HPC partitions have the Modules utility to
let you dynamically control your environment. Modules allows you to load
and unload packages and ensure a coherent working environment. The most
basic Modules commands let you add and remove packages from your
environment:

::

    module load <package name>/<optional package version>
    module unload <package name>/<optional package version>

To display the list of available modules:

::

    module avail

To display the list of currently loaded modules:

::

    module list

It is very important to make sure the proper modules are loaded in the
environment before you try to use FutureGrid HPC partitions. This
ensures that your $PATH, $LD\_LIBRARY\_PATH, $LD\_PRELOAD and other
environment variables are properly set and that you can access the
programs and libraries you need. Additional information about the
Modules utility is available via 'man module' on any FutureGrid login
node.
 

Managing Applications with Torque
---------------------------------

To run any jobs on resources within FutureGrid HPC partitions (single
core, OpenMP or MPI jobs), users must use the job scheduler and a job
submission script. Users should NOT run jobs on the login or headnodes.
On FutureGrid machines, the job scheduler is the Torque (a variant of
PBS). To load torque into your environment, execute:

::

    -bash-3.2$ module load torque


To run a serial job, you start by creating a job submission script
that both describes your job and will be executed on the compute nodes
by Torque. An example of a job script for India, Sierra, and Alamo that
runs */bin/hostname* is:


::

    #!/bin/bash

    #PBS -N hostname_test
    #PBS -o hostname.out
    #PBS -e hostname.err
    #PBS -q short
    #PBS -l nodes=1
    #PBS -l walltime=00:20:00

    /bin/hostname

Options are passed to Torque on lines that begin with #PBS. The options
above are:

-  -N: An optional job name
-  -o: The name of the file to write stdout to
-  -e: The name of the file to write stderr to
-  -q: The queue to submit the job to
-  -l: The resources needed by the job (in the case above, 1 node for 20
   minutes)

Additional information about the options that can be specified in a
submit script is available in the qsub manual page via 'man qsub'. Note
that there are multiple queues available on each FutureGrid system:

-  Alamo: short, long, default
-  Hotel: extended, batch, long and route
-  India: scalemp, batch, long and b534
-  Sierra: batch and long
-  Xray: batch

You can find information (such as limits) that will help you select
which queue to use by running qstat -q on the login node for the system
you are interested in.

Once you have created a submission script, you can then use the Torque
qsub command to submit this job to be executed on the compute nodes:

::

    -bash-3.2$ qsub ring.sh
    19095.master1.cm.cluster

The qsub command outputs either a job identifier or an error message
describing why Torque would not accept your job. If your job is
submitted successfully, you can track its execution using the qstat
command:

::

    -bash-3.2$ qstat
    Job id                    Name             User            Time Use S Queue
    ------------------------- ---------------- --------------- -------- - -----
    ...
    19095.master1             hostname_test    user            00:00:00 R short
    ...

If the system is busy, your job will initially be queued (Q) waiting for
resources to become available. It will then be in the running state (R),
and finally it will complete and not be visible in the qstat output. The
full set of Torque job states is provided in the qstat manual page via
man qstat on a FutureGrid login node. The stdout and stderr from your
job will be placed in the files you specified in your submission script.

A final Torque command you will use occasionally is the qdel command
that asks Torque to delete a job. If the job hasn't begun running, it is
simply deleted from the queue. If the job has begun, it is killed on the
nodes it's running on, and deleted from the queue.

A list of all available Torque commands is available from the `Torque
manual page <http://www.clusterresources.com/torquedocs21/>`__.

 

Message Passing Interface (MPI)
-------------------------------

The Message Passing Interface Standard (MPI) is a message passing
library standard based on the consensus of the MPI Forum, which has
dozens of participating organizations, including vendors, researchers,
software library developers, and users. The goal of the Message Passing
Interface is to establish a portable, efficient, and flexible standard
for message passing that will be widely used for writing message passing
programs. MPI is the *de facto* standard communication library for
almost all HPC systems, and is available in a variety of
implementations.

For more information, please visit:

-  `http://www.mpi-forum.org/ <http://www.mpi-forum.org/>`__
-  `http://www.mcs.anl.gov/research/projects/mpi/tutorial/ <http://www.mcs.anl.gov/research/projects/mpi/tutorial/>`__


For more information on OpenMPI, the default MPI distribution on
FutureGrid, please visit:

-  `http://www.open-mpi.org/ <http://www.open-mpi.org/>`__



MPI Libraries
-------------


The FutureGrid systems that support HPC-style usage have an MPI
implementation. In most cases, it is OpenMPI-1.4.x compiled with Intel
11.1 compilers. 


+--------------+-------------------+----------------+--------------------------+-----------------------------+
| **System**   | **MPI version**   | **Compiler**   | **Infiniband Support**   | **Module**                  |
+--------------+-------------------+----------------+--------------------------+-----------------------------+
| Alamo        | OpenMPI 1.4.3     | Intel 11.1     | yes                      | openmpi                     |
+--------------+-------------------+----------------+--------------------------+-----------------------------+
| Bravo        | OpenMPI 1.4.2     | Intel 11.1     | no                       | openmpi                     |
+--------------+-------------------+----------------+--------------------------+-----------------------------+
|              | OpenMPI 1.4.3     | gcc 4.4.6      | no                       | openmpi/1.4.3-gnu           |
+--------------+-------------------+----------------+--------------------------+-----------------------------+
|              | OpenMPI 1.4.3     | Intel 11.1     | no                       | openmpi/1.4.3-intel         |
+--------------+-------------------+----------------+--------------------------+-----------------------------+
|              | OpenMPI 1.5.4     | gcc 4.4.6      | no                       | openmpi/1.5.4-[gnu,intel]   |
+--------------+-------------------+----------------+--------------------------+-----------------------------+
| Hotel        | OpenMPI 1.4.3     | gcc 4.1.2      | yes                      | openmpi                     |
+--------------+-------------------+----------------+--------------------------+-----------------------------+
| India        | OpenMPI 1.4.2     | Intel 11.1     | yes                      | openmpi                     |
+--------------+-------------------+----------------+--------------------------+-----------------------------+
| Sierra       | OpenMPI 1.4.2     | Intel 11.1     | no                       | openmpi                     |
+--------------+-------------------+----------------+--------------------------+-----------------------------+
| Xray         |                   |                | N/A                      |                             |
+--------------+-------------------+----------------+--------------------------+-----------------------------+

| 
In cases where the OpenMPI is compiled with the Intel compilers,
loading the OpenMPI module will automatically load the Intel compilers
as a dependency:

::

    -bash-3.2$ module load openmpi
    Intel compiler suite version 11.1/072 loaded
    OpenMPI version 1.4.3 loaded

| Loading the OpenMPI module adds the MPI compilers to your $PATH
environment variable and the OpenMPI shared library directory to your
$LD\_LIBRARY\_PATH. This is an important step to ensure MPI applications
will compile and run successfully. Loading the torque module allows you
to submit jobs to the scheduler.
 

Compiling MPI Applications
--------------------------

To compile MPI applications, users have two options:

#. Use the MPI compilers instead of regular Intel/GNU compilers
#. Use the regular compilers (Intel/GNU) with MPI compilation flags

We recommend using the MPI compilers to avoid compilation issues. This
is accomplished by making the following replacements:

-  CC/icc/gcc with mpicc
-  CXX/icpc/g++ with mpicxx
-  F90/F77/FC/ifort/gfortran with mpif90

Alternatively, for some codes that require intricate compilation flags
and complicated make systems, and where changing compilers is not an
option, you can edit the compilation/linking options for your codes.
These options are machine, compiler, and language dependent. To view the
options required for C, C++ and Fortran on any machine, you can issue
the commands mpicc-show, mpicxx-show, and mpif90-show. Extra care must
be taken when using these flags, as dependencies govern the order in
which they appear in the link line. Should you run into compilation
errors or problems, please submit a consulting ticket.

Assuming you have loaded the openmpi module into your environment,
you can compile a `simple MPI application </tutorials/hpc/ring>`__ as
easily as executing:

::

    -bash-3.2$ mpicc -o ring ring.c



Running MPI Applications
------------------------

Once your MPI application is compiled, you run it on the compute nodes
of a cluster via Torque. An example of an MPI parallel job script for
India, Sierra, and Alamo that runs the ring application is:


::

    #!/bin/bash

    #PBS -N ring_test
    #PBS -o ring_$PBS_JOBID.out
    #PBS -e ring_$PBS_JOBID.err
    #PBS -q short
    #PBS -l nodes=4:ppn=8
    #PBS -l walltime=00:20:00

    # make sure MPI is in the environment
    module load openmpi

    # launch the parallel application with the correct number of processs
    # Typical usage: mpirun -np <number of processes> <executable> <arguments>
    mpirun -np 32 ring -t 1000

There are two important differences between this script and the submit
script shown previously. The first is that :ppn=8 is added to the
request for four nodes. What this does is indicate that your application
wants to allocate eight virtual processors per node. A virtual processor
corresponds to a processing core. Alamo, Hotel, India, and Sierra all
have eight cores per node, so the script above asks for exclusive access
to four nodes with a total of 32 cores. The second importand difference
from the previous submit script is that it executes mpirun with
arguments that describe your MPI application. Note that the number of
processes specified to mpirun is 32—matching the 32 cores allocated by
Torque.

A minor difference between this script and the previous one is that
the environment variable $PBS\_JOBID is used when creating the stdin and
stdout files. Torque sets a number of environment variables that you can
use in your submit script, starting with PBS\_ .

Log in to HPC services
======================

 

To access a FutureGrid system via Torque/Moab, you should ssh to the
login node for the system. The login node is one of the following:

-  india.futuregrid.org
-  bravo.futuregrid.org
-  sierra.futuregrid.org
-  foxtrot.futuregrid.org
-  hotel.futuregrid.org
-  alamo.futuregrid.org
-  xray.futuregrid.org

An example session follows:

    ::

        $ ssh sierra.futuregrid.org
        Last login: Thu Aug 12 19:19:22 2010 from ....
        Welcome to Sierra.FutureGrid.Org
        $

Once you ssh into these nodes, you'll have access to the HPC queuing
services for the machine you have logged into. You will enter into a
Unix/Linux shell in which you can enter the typical Unix commands.
However, access to the clusters is provided through Torque/Moab commands
from the command line. 

Generating SSH Keys for FutureGrid Access
=========================================

 

Key Generation
==============

To gain access to FutureGrid Resources including HPC and Nimbus
services, you need to provide a public key to FutureGrid. We recommend
that you are familiar with public keys and have the understanding that
we do REQUIRE passphrase protected keys. To generate and send such a
key, please follow the following steps. To find more out about open
ssh you can also go to 

-  `http://openssh.com/manual.html <http://openssh.com/manual.html>`__

Other good resources include 

-  http://help.github.com/key-setup-redirect
-  http://help.github.com/working-with-key-passphrases/
-  http://www.dribin.org/dave/blog/archives/2007/11/28/ssh\_agent\_leopard/

**1. **\ **Generate Public/Private Key Pair**
---------------------------------------------

First, you have to generate a key. You do this as follows:

-  Step 1: use the command “ssh-keygen -t rsa -C <your-e-mail>” to
   generate the key
-  Step 2: specifiy the KeyPair location and name. We recommend that you
   use the default location if you do not yet have another key there.
    e.g. /home/username/.ssh/id\_rsa
-  Step 3: type user defined passphrase when asking passphrase for the
   key

Example:

::

    ssh-keygen -t rsa -C johndoe@indiana.edu

    Generating public/private rsa key pair.
    Enter file in which to save the key (/home/johndoe/.ssh/id_rsa): 
    Enter passphrase (empty for no passphrase):
    Enter same passphrase again:
    Your identification has been saved in /home/johndoe/.ssh/id_rsa.
    Your public key has been saved in /home/johndoe/.ssh/id_rsa.pub.
    The key fingerprint is:
    34:87:67:ea:c2:49:ee:c2:81:d2:10:84:b1:3e:05:59 johndoe@indiana.edu

**2. **\ **List the result**
----------------------------

You can find your key under the key location. As we user the .ssh
directory it will be located there. 
 

::

    $ls -lisa ~/.ssh
    -rw-------  1 johndoe johndoe        1743 2011-02-10 09:44 id_rsa
    -rw-r--r--  1 johndoe johndoe         399 2011-02-10 09:44 id_rsa.pub

 

**3. **\ **Add or Replace Passphrase for an Already Generated Key**
-------------------------------------------------------------------

In case you need to change your change passphrase, you can simply run
“ssh-keygen -p” command. Then specify the location of your current key,
and input (old and) new passphrases. There is no need to re-generate
keys.

::

    ssh-keygen -p

    Enter file in which the key is (/home/johndoe/.ssh/id_rsa):
    Enter old passphrase:
    Key has comment '/home/johndoe/.ssh/id_rsa'
    Enter new passphrase (empty for no passphrase):
    Enter same passphrase again:
    Your identification has been saved with the new passphrase.  

 

**4.**  **Capture the Public Key for FutureGrid**
-------------------------------------------------

Use a text editor to open the “id\_rsa.pub”. Copy the **entire**
contents of this file into the ssh key field as part of your profile
information. You can now add this key to your keys at the following
page:

5. Key Management
-----------------

This is a future section that will include material about how to use
ssh-add and keychain. You can find instructions on them via the
github link that we provided above. However, we are not github ;-)

6. Resetting the SSH key
------------------------

Please follow the instructions given at 

-  `https://portal.futuregrid.org/how-do-i-reset-my-ssh-key <https://portal.futuregrid.org/how-do-i-reset-my-ssh-key>`__

 

7. I still can not access FG resources
--------------------------------------

In order for you to access FG resources you must be in an active
project. Please make sure you join a project or create your own while
applying for one.
 

 

 
=

 

Working with HPC Job Services
=============================

 

Running Queued Jobs as Part of the HPC Services
-----------------------------------------------

To run a job in the HPC service, you need to create a job script that
tells the job manager how to run the job and how to handle things like
output and notifications. You can then submit your job to the scheduler,
monitor its progress in the job queue, and examine the output when it
finishes.

 A Simple Job Script
~~~~~~~~~~~~~~~~~~~~

An example job script looks like this:

    ::

        #!/bin/bash
        #PBS -N testjob 
        #PBS -l nodes=1:ppn=1 
        #PBS -q batch 
        #PBS -j oe 
        ##PBS -M username@example.com 
        ##PBS -m ae ##PBS -o testjob.out 
        ## 
        ## Everything following is run by the scheduler 
        ## 
        sleep 10 
        echo -n "Host operating system version: " 
        uname -a 
        echo "Nodes allocated to this job: " 
        cat $PBS_NODEFILE 
        echo 
        sleep 10 
        ## 
        ## End of job script 
        ##

In the job script, lines that begin with \ **#PBS** are directives to
the job scheduler. You can disable any of these lines by adding an
extra \ **#**\ character at the beginning of the line, for example:

    ::

        ##PBS -M username@example.com

This job script shows some common examples of directives that you might
want to use in your job scripts. The directives in this job script are
described below:

    ::

        #!/bin/bash 

This line isn't strictly required, but it is added as a fail-safe in
case something unexpected happens. Normally, the job manager reads your
script and processes the directives, and then runs your script as a
normal shell script. This simply ensures that the system uses the
standard bash shell to run your script.

    ::

        #PBS -N testjob 

This line gives your job a name of \ **testjob**. This name will be used
by the job manager when it shows a job listing, and will be used for
your output file(s) unless you explicitly specify an output file.

    ::

        #PBS -l nodes=1:ppn=1 

This line tells the job manager what your job requires for resources. In
this case, your job is asking for one node (**nodes=1**) and at least
one processor per node (**ppn=1**). See the
[[Sw:Manual/PBSDirectives\|PBSDirectives] page for other options you can
specify here.

    ::

        #PBS -q batch 

This line tells the job manager which job queue your job should be sent
to. Each job queue has different characteristics, such as the maximum
time a job is allowed to run, or the maximum number of nodes a job can
use.

    ::

        #PBS -j oe 

This line tells the job manager to join the job standard output and
standard error into a single file. For jobs with a small amount of
output, this is usually helpful. If your job produces a lot of standard
output, it may be helpful to keep the files separate so you can easily
locate error messages in the single error file.

    ::

        ##PBS -M username@example.com 

Note that this line is a comment since it starts with \ **##** instead
of \ **#PBS**. If you remove the first \ **#**, this line will set the
email address that will get notified about events related to this job.
The events that get reported are set by the next line.

    ::

        ##PBS -m ae 

Again, note that this line is commented out. If you remove the
first \ **#**, this line will send email whenever the job fails
(or **a**\ borts) (**a** option), and when the job ends (**e** option).
This is particularly helpful if your job has to wait a long time in the
queue before it runs.

    ::

        ##PBS -o testjob.out 

Again, note that this line is commented out. If you remove the
first \ **#**, this line will specify the file name to be used for job
output.

  Submitting Your Job
~~~~~~~~~~~~~~~~~~~~~

You can submit your job with the \ **qsub** or **msub** commands.
The \ **msub** and **qsub** are almost identical, and can mostly be used
interchangeably. See the respective man pages for specific differences.
Neither submission command provides much output. Examples of a job
submission using both commands follows:

Using \ **msub**:

    ::

        [62]s1::gpike> msub testjob.pbs 
        292250 
        [63]s1::gpike>

Using \ **qsub**:

    ::

        [63]s1::gpike> qsub testjob.pbs 
        292251.s82 
        [64]s1::gpike>

In both cases, the number that gets returned is the job number that the
scheduler assigned to your job. In the case of \ **qsub**, the job
number is followed by the host name where you submitted the job.

Monitoring Your Job
-------------------

To monitor your job after it has been submitted, you can use
the \ **qstat** or **showq** commands. Both commands will show you the
state of the job manager, but the information is displayed in different
formats. In general, the \ **showq** command gives more complete
information, and in a form that is a bit easier to read.
The \ **qstat** command gives a very concise listing of the job queue,
and in some instances this may give you a better quick overview of the
resource.

Using the test job script as an example, here is the output from
the \ **showq** command:

    ::

        [66]s1::gpike> showq 
        active jobs
        ------------------------ 
        JOBID    USERNAME       STATE PROCS    REMAINING            STARTTIME 
        292252   greg       Running     16        3:59:59 Tue Aug 17 09:02:40 
        1 active job 16 of 264 processors in use by local jobs (6.06%) 
                          2 of 33 nodes active (6.06%) eligible jobs
        ----------------------
        JOBID    USERNAME       STATE PROCS    REMAINING            STARTTIME
        0 eligible jobs blocked jobs
        ----------------------- 
        JOBID    USERNAME       STATE PROCS    REMAINING            STARTTIME
        0 blocked jobs 
        Total job: 1 

You can see the output is divided into three sections: \ **active
jobs**, \ **eligible jobs**, and \ **blocked jobs**.

**1. Active jobs** are jobs that are currently running on the resource.

**2.**\ **Eligible jobs** are jobs that are waiting for nodes to become
available before they can run. As a general rule, jobs are listed in the
order that they will be scheduled, but scheduling algorithms may change
the order over time.

**3.**\ **Blocked jobs** are jobs that the scheduler cannot run for some
reason. Usually a job becomes blocked because it is requesting something
that is impossible, such as more nodes than currently exist, or more
processors per node than are installed.

 

Using the test job as an example again, here is the output from
the \ **qstat** command:

    ::

        [109]i136::gpike> qstat 
        Job id                             Name               User          Time Use S Queue 
        ------------------------- --------------------- ------------------- -------- - ----- 
        1981.i136                       sub19327.sub      inca               00:00:00 C batch 
        1982.i136                       testjob           greg                      0 R batch 

The \ **qstat** command provides output in six columns:

#. Job id is the identifier assigned to your job.
#. Name is the name that you assigned to your job.
#. User is the username of the person who submitted the job.
#. Time Use is the amount of time the job has been running.
#. S shows the job state. Common job states are R for a running job, Q
   for a job that is queued and waiting to run, C for a job that has
   completed, and H for a job that is being held.
#. Queue is the name of the job queue where your job will run.

 

Examining Your Job Output
-------------------------

If you gave your job a name with the \ **#PBS -N <jobname>** directive
in your job script or by specifying the job name on the command line,
your job output will be available in a file named \ **jobname.o######**,
where the \ **######** is the job number assigned by the job manager.
You can type \ **ls jobname.o\*** to see all output files from the same
job name.

If you explicitly name an output file with the \ **#PBS -o
<outfile>** directive in your job script or by specifying the output
file on the command line, your output will be in the file you specified.
If you run the job again, the output file will be overwritten.

If you don't specify any output file, your job output will have the same
name as your job script, and will be numbered in the same manner as if
you had specified a job name (**jobname,o######**).



Xray
====

IU Cray User Manual
-------------------

  Hostname
^^^^^^^^^^

• xray.futuregrid.org

  Login
^^^^^^^

    ::

        ssh xray.futuregrid.org

 Filesystem
^^^^^^^^^^^

 Compiler
^^^^^^^^^

For MPI jobs, use cc (pgcc).

For best performance, add the xtpe-barcelona module 

    ::

        % module add xtpe-module

 Cray Programming Environment Manuals
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

-  `http://docs.cray.com/cgi-bin/craydoc.cgi?q=&mode=Search&hw=%22Cray+XT5%22 <http://docs.cray.com/cgi-bin/craydoc.cgi?mode=View;id=S-2396-21>`__
-  `http://docs.cray.com/cgi-bin/craydoc.cgi?mode=View;id=S-2396-21 <http://docs.cray.com/cgi-bin/craydoc.cgi?mode=View;id=S-2396-21>`__

  Queue
^^^^^^^

Currently there is only one queue (batch) available to users on the
Cray, and all jobs are automatically routed to that queue.

 Listing Queues on Xray
^^^^^^^^^^^^^^^^^^^^^^^

    ::

         qstat -Q

The primary queue for running jobs on Xray is batch. To obtain details
of running jobs and available processors, use the showq command.

    ::

        /opt/moab/default/bin/showq

 Submitting a job
~~~~~~~~~~~~~~~~~

**MPI run cmd**:  aprun

Example \ **job script (16 processors / 2 nodes):**

    ::

        % cat job.sub 

    ::

        #!/bin/sh
        #PBS -l mppwidth=16 
        #PBS -l mppnppn=8 
        #PBS -N hpcc-16 
        #PBS -j oe 
        #PBS -l walltime=7:00:00 
        #cd to directory where job was submitted from 
        cd $PBS_O_WORKDIR 
        export MPICH_FAST_MEMCPY=1 
        export MPICH_PTL_MATCH_OFF=1 
        aprun -n 16 -N 8 -ss -cc cpu hpcc
        % qsub job.sub 

 Looking at the Queue

    ::

        % qstat

How Do I Submit a Job to the Cray XT5m on FutureGrid?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

`http://kb.iu.edu/data/azse.html <http://kb.iu.edu/data/azse.html>`__

The XT5m is a 2D mesh of nodes. Each node has two sockets, and each
socket has four cores.

The batch scheduler interfaces with a Cray resource scheduler called
APLS. When you submit a job, the batch scheduler talks to ALPS to find
out what resources are available, and ALPS then makes the reservation.

Currently ALPS is a "gang scheduler" and only allows one "job" per node.
If a user submits a job in the format aprun -n 1 a.out , ALPS will put
that job on one core of one node and leave the other seven cores empty.
When the next job comes in, either from the same user or a different
one, it will schedule that job to the next node.

If the user submits a job with aprun -n 10 a.out , then the scheduler
will put the first eight tasks on the first node and the next two tasks
on the second node, again leaving six empty cores on the second node.
The user can modify the placement with -N , -S , and -cc .

A user might also run a single job with multiple treads, as with OpenMP.
If a user runs this job aprun -n 1 -d 8 a.out , the job will be
scheduled to one node and have eight threads running, one on each core.

You can run multiple, different binaries at the same time on the same
node, but only from one submission. Submitting a script like this
will not work:

    ::

        OMP_NUM_THREADS=1 aprun -n 1 -d 1 -cc 0 ./my-binary
        OMP_NUM_THREADS=1 aprun -n 1 -d 1 -cc 1 ./my-binary
        OMP_NUM_THREADS=1 aprun -n 1 -d 1 -cc 2 ./my-binary
        OMP_NUM_THREADS=1 aprun -n 1 -d 1 -cc 3 ./my-binary
        OMP_NUM_THREADS=1 aprun -n 1 -d 1 -cc 4 ./my-binary
        OMP_NUM_THREADS=1 aprun -n 1 -d 1 -cc 5 ./my-binary
        OMP_NUM_THREADS=1 aprun -n 1 -d 1 -cc 6 ./my-binary
        OMP_NUM_THREADS=1 aprun -n 1 -d 1 -cc 7 ./my-binary

This will run a job on each core, but not at the same time. To run all
jobs at the same time, you need to first bury all the binaries under
one aprun command:

    ::

        $ more run.sh
        ./my-binary1
        ./my-binary2
        ./my-binary3
        ./my-binary4
        ./my-binary5
        ./my-binary6
        ./my-binary7
        ./my-binary8
        $ aprun -n 1 run.sh

Alternatively, use the command aprun -n 1 -d 8 run.sh . To run multiple
serial jobs, you must build a batch script to divide the number of jobs
into groups of eight, and the

Alamo
=====

Alamo is a 96 node Dell cluster running 2.66 GHz Intel Xeon X5550
processors.  The OS is CentOS 5.8 and 6.3.  It runs Torque and Moab for
scheduling.  Alamo has a QDR IB interconnect and 15 TB of attached disk
storage.   Alamo is partitioned into different resources for Nimbus and
HPC.  See the Alamo hardware page for more
detail: \ `https://portal.futuregrid.org/hardware/alamo <https://portal.futuregrid.org/hardware/alamo>`__
.

**Nimbus partition -** see nimbus
documentation: \ `https://portal.futuregrid.org/tutorials/nimbus <https://portal.futuregrid.org/tutorials/nimbus>`__

**HPC partition**

Max cores 584.  Submit
`ticket <http://%20https://portal.futuregrid.org/help>`__ if you need to
run larger than the 320 limit per user. 

Available queues: 
   short - 24 hours runtime limit
   long - 72 hours runtime limit

After registering your .ssh key on the portal, go to
`https://portal.futuregrid.org/manual/access <https://portal.futuregrid.org/manual/access>`__
.

If you key has been uploaded, you can ssh to the login node using the
following command. 
**Note**: If you are prompted for a password, your account has not
been set up correctly or the .ssh key has not been propagated. 

  ssh alamo.futuregrid.org

To submit a test job use the qsub command.

ex.  qsub -N job\_name -l nodes=1 -q short  job\_script

-  qstat - show current jobs in the queue with status
-  showq - show current running and queued jobs and job id
-  checkjob -v <jobid>   - more detailed information about your job

Applications are available via modules.  To see a list of available
applications:
  module avail

File systems:
   /home   - Quota enforced home directory, backed up nightly.
   /N/work - 6.3 TB work directory, not backed up. NFS mounted from
login node. 
   /N/images - 11 TB directory for system images, not backed up. NFS
mounted from login node.

Administrator: David Gignac
For issues or questions please
use \ `https://tickets.futuregrid.org <https://tickets.futuregrid.org>`__
. You can conveniently submit a ticket
via \ `https://portal.futuregrid.org/help <https://portal.futuregrid.org/help>`__.
To look at your previously submitted tickets you can
use \ `https://portal.futuregrid.org/tickets <https://portal.futuregrid.org/tickets>`__
.

  



IaaS - Infrastructure as a Service
==================================

This chapter contains information in regards to Infrastructure as a
Service offerings on FutureGrid

Using IaaS Clouds on FutureGrid
===============================

Infrastructure-as-a-Service (IaaS) cloud computing encompasses
techniques that have driven major recent advances in information
technology supporting elastic, on-demand, "pay as you go" computing as a
service. Key technologies behind IaaS cloud computing are resource
virtualization, as well as cloud middleware that enables the management
of clusters of virtualized resources through service interfaces. 

The FutureGrid testbed provides capabilities that allow users to
experiment with open-source cloud middleware and virtualization
platforms, and there are different ways you may want to use these
platforms in the testbed. This page guides you in selecting from
FutureGrid capabilities best suited to your goals, and provides links to
respective tutorials:







Management Services
===================

FutureGrid contains a number of interresting management services. This
includes image management services to deploy and provision images onto
bare metal or virtualized machines as well as experiment management that
allows the creation of easy to use workflows to run repeatable
experiments on FutureGrid. These services are curently under development
and you are welcome to join the development teams by contacting
`laszewski@gmail.com <mailto:laszewski@gmail.com>`__



.. |image0| image:: https://portal.futuregrid.org/sites/default/files/u30/fg-logo-md.gif
.. |image4| image:: https://portal.futuregrid.org/sites/default/files/resize/images/FutureGrid_iDataPlex_Cray_IU-sm-640x425.jpg
.. |image5| image:: https://portal.futuregrid.org/sites/default/files/resize/images/Cray_XT5m_Front_closed-small-427x640.jpg
.. |image6| image:: https://portal.futuregrid.org/sites/default/files/images/FutureGrid%20Logocal%20v3.png
.. |image7| image:: https://portal.futuregrid.org/sites/default/files/resize/images/Juniper%20EX8208-140x184.png
.. |image8| image:: https://portal.futuregrid.org/sites/default/files/u23/futuregrid-physical.png
.. |image9| image:: https://portal.futuregrid.org/sites/default/files/u23/futuregrid-topology.png
.. |image10| image:: https://portal.futuregrid.org/sites/default/files/images/Spirent%20XGEM.png
.. |image11| image:: https://portal.futuregrid.org/sites/default/files/images/FutureGrid%20Logocal%20v3.png
.. |image12| image:: https://portal.futuregrid.org/sites/default/files/images/status_incapart.PNG
   :target: http://inca.futuregrid.org:8080/inca/jsp/partitionTable.jsp
.. |image13| image:: https://portal.futuregrid.org/sites/default/files/ScreenSnapz.jpg
   :target: http://inca.futuregrid.org:8080/inca/jsp/status.jsp?queryNames=Health&xsl=table.xsl&resourceIds=FutureGrid
.. |image14| image:: https://portal.futuregrid.org/sites/default/files/ganglia.png
   :target: http://ganglia.futuregrid.org
.. |image15| image:: https://portal.futuregrid.org/sites/default/files/images/large_status_nocmap.PNG
   :target: http://noc.futuregrid.org
.. |image16| image:: https://portal.futuregrid.org/sites/default/files/u23/Screen%20shot%202011-01-14%20at%207.48.06%20PM.png
   :target: http://inca.futuregrid.org
.. |image17| image:: https://portal.futuregrid.org/sites/default/files/u23/Screen%20shot%202011-04-07%20at%203.23.05%20PM.png
   :target: https://portal.futuregrid.org/monitoring/cloud
.. |image18| image:: https://portal.futuregrid.org/sites/default/files/screenshot-for-status-small.png
   :target: https://portal.futuregrid.org/metrics
.. |image19| image:: https://portal.futuregrid.org/sites/default/files/fg-sys-sw-ver.PNG
   :target: http://inca.futuregrid.org:8080/inca/HTML/rest/HPC/FutureGrid
.. |image20| image:: https://portal.futuregrid.org/sites/default/files/u15/nimbus-usage.png
   :target: http://inca.futuregrid.org/nimbus-stats
.. |image21| image:: https://portal.futuregrid.org/sites/default/files/u30/cygwim1.png
.. |image22| image:: https://portal.futuregrid.org/sites/default/files/u30/cygwin2.png
.. |image23| image:: https://portal.futuregrid.org/sites/default/files/u30/cygwinfirst.png
.. |image24| image:: https://portal.futuregrid.org/sites/default/files/u23/register-sshkey.png
   :target: https://portal.futuregrid.org/user/23/ssh-keys
.. |image25| image:: https://portal.futuregrid.org/sites/default/files/u30/portalLogin_0.png
.. |image26| image:: https://portal.futuregrid.org/sites/default/files/u30/portalsshkey.png
.. |image27| image:: https://portal.futuregrid.org/sites/default/files/u30/portalclikaddkey_0.png
.. |image28| image:: https://portal.futuregrid.org/sites/default/files/u30/portalkeypaste_0.png
.. |image29| image:: https://portal.futuregrid.org/sites/default/files/u30/icl_footer.gif
.. |image30| image:: https://portal.futuregrid.org/sites/default/files/images/otf_0.png
.. |image31| image:: https://portal.futuregrid.org/sites/default/files/images/open_file.png
.. |image32| image:: https://portal.futuregrid.org/sites/default/files/images/cancel_loading_resize.png
.. |image33| image:: https://portal.futuregrid.org/sites/default/files/images/Startup.png
.. |image34| image:: https://portal.futuregrid.org/sites/default/files/images/Display_arranging_a.png
.. |image35| image:: https://portal.futuregrid.org/sites/default/files/images/Display_arranging_b.png
.. |image36| image:: https://portal.futuregrid.org/sites/default/files/images/Custom_arrangement.png
.. |image37| image:: https://portal.futuregrid.org/sites/default/files/images/close_display.png
.. |image38| image:: https://portal.futuregrid.org/sites/default/files/images/Undocking_1.png
.. |image39| image:: https://portal.futuregrid.org/sites/default/files/images/Undocking_2.png
.. |image40| image:: https://portal.futuregrid.org/sites/default/files/images/Resize_labels.png
.. |image41| image:: https://portal.futuregrid.org/sites/default/files/images/Zooming.png
.. |image42| image:: https://portal.futuregrid.org/sites/default/files/images/Zoom_toolbar.png
.. |image43| image:: https://portal.futuregrid.org/sites/default/files/images/icon_master_tl.png
.. |image44| image:: https://portal.futuregrid.org/sites/default/files/images/icon_process_tl.png
.. |image45| image:: https://portal.futuregrid.org/sites/default/files/images/icon_counter_tl.png
.. |image46| image:: https://portal.futuregrid.org/sites/default/files/images/icon_radar.png
.. |image47| image:: https://portal.futuregrid.org/sites/default/files/images/icon_function_summ.png
.. |image48| image:: https://portal.futuregrid.org/sites/default/files/images/icon_message_summ.png
.. |image49| image:: https://portal.futuregrid.org/sites/default/files/images/icon_process_summ.png
.. |image50| image:: https://portal.futuregrid.org/sites/default/files/images/icon_matrix.png
.. |image51| image:: https://portal.futuregrid.org/sites/default/files/images/icon_calltree.png
.. |image52| image:: https://portal.futuregrid.org/sites/default/files/images/icon_legend.png
.. |image53| image:: https://portal.futuregrid.org/sites/default/files/images/icon_context.png
.. |image54| image:: https://portal.futuregrid.org/sites/default/files/images/icon_marker.png
.. |image55| image:: https://portal.futuregrid.org/sites/default/files/images/Master_timeline.png
.. |image56| image:: https://portal.futuregrid.org/sites/default/files/images/Process_timeline.png
.. |image57| image:: https://portal.futuregrid.org/sites/default/files/images/collectives.png
.. |image58| image:: https://portal.futuregrid.org/sites/default/files/images/burst.png
.. |image59| image:: https://portal.futuregrid.org/sites/default/files/images/marker-multiple.png
.. |image60| image:: https://portal.futuregrid.org/sites/default/files/images/marker-template.png
.. |image61| image:: https://portal.futuregrid.org/sites/default/files/images/io-multiple.png
.. |image62| image:: https://portal.futuregrid.org/sites/default/files/images/io-single.png
.. |image63| image:: https://portal.futuregrid.org/sites/default/files/images/io-single-selected.png
.. |image64| image:: https://portal.futuregrid.org/sites/default/files/images/Counter_data_timeline.png
.. |image65| image:: https://portal.futuregrid.org/sites/default/files/images/performance_radar_find_function.png
.. |image66| image:: https://portal.futuregrid.org/sites/default/files/images/performance_radar_set_counter.png
.. |image67| image:: https://portal.futuregrid.org/sites/default/files/images/Call_tree.png
.. |image68| image:: https://portal.futuregrid.org/sites/default/files/images/Function_summary.png
.. |image69| image:: https://portal.futuregrid.org/sites/default/files/images/Process_summary.png
.. |image70| image:: https://portal.futuregrid.org/sites/default/files/images/Messagesummary.png
.. |image71| image:: https://portal.futuregrid.org/sites/default/files/images/Communication_matrix_view.png
.. |image72| image:: https://portal.futuregrid.org/sites/default/files/images/Function_legend.png
.. |image73| image:: https://portal.futuregrid.org/sites/default/files/images/Marker_view.png
.. |image74| image:: https://portal.futuregrid.org/sites/default/files/images/Context_view.png
.. |image75| image:: https://portal.futuregrid.org/sites/default/files/images/context_compare.png
.. |image76| image:: https://portal.futuregrid.org/sites/default/files/images/process_filter.png
.. |image77| image:: https://portal.futuregrid.org/sites/default/files/images/pref_general.png
.. |image78| image:: https://portal.futuregrid.org/sites/default/files/images/pref_appearance.png
.. |image79| image:: https://portal.futuregrid.org/sites/default/files/resize/u28/CUBLAS2-800x280.png
.. |image80| image:: https://portal.futuregrid.org/sites/default/files/resize/u28/cudaarchi_threadsmode-544x300.png
.. |image81| image:: https://portal.futuregrid.org/sites/default/files/resize/u28/cudaMemoryArchitecture-500x173.png
.. |image82| image:: https://portal.futuregrid.org/sites/default/files/u28/cmeans_objective_function.gif
.. |image83| image:: https://portal.futuregrid.org/sites/default/files/u28/cmeans_uij_ck.gif
.. |image84| image:: https://portal.futuregrid.org/sites/default/files/u28/cmeans_ck.gif
.. |image85| image:: https://portal.futuregrid.org/sites/default/files/u28/cmeans_stop_condition.gif
.. |image86| image:: https://portal.futuregrid.org/sites/default/files/resize/u28/cmeansPerformance2-600x178.png
.. |image87| image:: https://portal.futuregrid.org/sites/default/files/images/nimbus_logo.png
   :target: http://www.nimbusproject.org/
.. |image88| image:: https://portal.futuregrid.org/sites/default/files/resize/euca_fg_login-290x240.png
.. |image89| image:: https://portal.futuregrid.org/sites/default/files/resize/u23/Screen%20Shot%202013-03-06%20at%2012.47.32%20PM-201x200.png
   :target: http://futuregrid.github.com/rain/
.. |image90| image:: https://portal.futuregrid.org/sites/default/files/resize/u23/Screen%20Shot%202013-03-06%20at%2012.51.48%20PM-200x200.png
   :target: http://futuregrid.github.com/rain/quickstart.html
.. |image91| image:: https://portal.futuregrid.org/sites/default/files/resize/u23/Screen%20Shot%202013-03-06%20at%2012.47.32%20PM-201x200.png
   :target: http://futuregrid.github.com/rain/
.. |image92| image:: https://portal.futuregrid.org/sites/default/files/resize/u23/Screen%20Shot%202013-03-06%20at%2012.51.48%20PM-200x200.png
   :target: http://futuregrid.github.com/rain/quickstart.html
.. |:!:| image:: http://www.opennebula.org/lib/images/smileys/icon_exclaim.gif
.. |image94| image:: https://portal.futuregrid.orghttps://portal.futuregrid.org/sites/default/files/resize/myHadoop-300x70.png
.. |Hadoop logo| image:: http://hadoop.apache.org/images/hadoop-logo.jpg
.. |image96| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image97| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image98| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image99| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image100| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image101| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image102| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image103| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image104| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image105| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image106| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image107| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image108| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image109| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image110| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image111| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image112| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image113| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image114| image:: http://www.iterativemapreduce.org/images/imrmodel.png
.. |image115| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image116| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image117| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image118| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image119| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image120| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image121| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image122| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image123| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image124| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image125| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image126| image:: http://www.iterativemapreduce.org/images/bullet.GIF
.. |image127| image:: https://portal.futuregrid.orghttps://portal.futuregrid.org/sites/default/files/u192/start_twister.jpg
.. |image128| image:: https://portal.futuregrid.orghttps://portal.futuregrid.org/sites/default/files/resize/u192/twister_kmeans-906x257.jpg
.. |Cloud site sample layouts.| image:: https://pegasus.isi.edu/wms/docs/4.0/images/fg-pwms-prefio.3.png
.. |image130| image:: https://portal.futuregrid.org/sites/default/files/u30/fg-logo-md.gif
.. |image131| image:: https://portal.futuregrid.org/sites/default/files/u30/fg-logo-md.gif
.. |image132| image:: https://portal.futuregrid.org/sites/default/files/u23/summerschool2012.png
   :target: https://portal.futuregrid.org/projects/241
