  
  <div class="content">
    <p>&nbsp;</p>
<p>&nbsp;</p>

	<h2>
		<a href="http://www.nimbusproject.org/"><img alt="" class="image image-_original " height="112" src="/sites/default/files/images/nimbus_logo.png" title="" width="200" /></a></h2>
	<p>&nbsp;</p>
	<h2>
		What is Nimbus?</h2>
	<p>Nimbus is an open source service package that allows users to run virtual machines on FutureGrid hardware. You can easily upload your own VM image or customize an image provided by us. When you boot a VM, it is assigned a public IP address (and/or an optional private address); you are authorized to log in as root via SSH. You can then run services, perform computations, and configure the system as desired. After using and configuring the VM, you can save the modified VM image back to the Nimbus image repository.</p>
	<p>&nbsp;</p>
	<h2>
		Nimbus on FutureGrid</h2>
	<p>Nimbus is installed on four FutureGrid clusters:</p>
	<ol>
		<li>
			<strong>Hotel</strong>&nbsp;(University of Chicago)<br />
			42 nodes,&nbsp;336&nbsp;cores</li>
		<li>
			<strong>Foxtrot</strong>&nbsp;(University of Florida)<br />
			24 nodes,&nbsp;192&nbsp;cores</li>
		<li>
			<strong>Sierra</strong>&nbsp;(San Diego Supercomputer Center)<br />
			18 nodes, 144 cores</li>
		<li>
			<strong>Alamo </strong>(Texas Advanced Computing Center)<br />
			15 nodes, 120 cores</li>
	</ol>
	<p>By default, users are limited to running 16 VMs simultaneously and claiming two cores per VM. If you have a good reason for this limitation to be lifted for your account, contact&nbsp;<a href="https://portal.futuregrid.org/help">https://portal.futuregrid.org/help</a>.</p>
	<p>All FutureGrid users are allowed access to Nimbus on all sites.</p>
	<h2>
		Getting Started</h2>
	<p>Nimbus provides services that can be controlled remotely using a variety of clients. In this tutorial, we will use a simple command line tool called the&nbsp;<strong>cloud-client</strong>. If you&#39;d rather have programmatic control, the Amazon EC2 protocols&nbsp;<a href="http://www.nimbusproject.org/docs/current/elclients.html">are supported</a>, which have a variety of excellent clients&nbsp;available&nbsp;for many languages.&nbsp;&nbsp;</p>
	<h3>
		Log into hotel</h3>
	<p>The first step is to ssh into hotel.futuregrid.org.&nbsp; While you can use Nimbus clients from anywhere in the world, we recommend that you start on hotel because the correct version of Java is installed there.</p>
	<pre>
$ ssh -A hotel.futuregrid.org</pre>
	<p>If this command fails, contact <a href="https://portal.futuregrid.org/help">https://portal.futuregrid.org/help</a>.&nbsp; It likely means one of the following:</p>
	<ol>
		<li>
			Your account is not yet set up.</li>
		<li>
			You provide no public key or a corrupted public key.</li>
		<li>
			The private key you are using does not match the public one you registered with FutureGrid.</li>
	</ol>
	<h3>
		Download and install cloud-client</h3>
	<p>Download the Nimbus cloud client from the&nbsp;<a href="http://www.nimbusproject.org/downloads/">Nimbus website</a>.</p>
	<pre>
$ wget http://www.nimbusproject.org/downloads/nimbus-cloud-client-021.tar.gz</pre>
	<p>Unpack the archive into your home directory.</p>
	<pre>
$ tar xzf nimbus-cloud-client-021.tar.gz
$ ls nimbus-cloud-client-021/
CHANGES.txt	README.txt	conf		lib
LICENSE.txt	bin		history		samples

</pre>
	<h3>
		Obtain Your Nimbus Credentials and Configuration Files</h3>
	<p>In your home directory on hotel, you will find the file nimbus_creds.tar.gz:</p>
	<pre>
 username@hotel $ ls ~/nimbus_creds.tar.gz
nimbus_creds.tar.gz
</pre>
	<p>If your credentials are not present on&nbsp;<strong>Hotel</strong>, contact&nbsp;<a href="https://portal.futuregrid.org/help">https://portal.futuregrid.org/help</a>.</p>
	<p>Download and unpack these files into your cloud-client&#39;s&nbsp;directory:</p>
	<pre>
$ cd nimbus-cloud-client-021/conf/&nbsp;
$ tar xvzf ~/nimbus_creds.tar.gz
usercert.pem
userkey.pem
cloud.properties
hotel.conf
sierra.conf
foxtrot.conf
alamo.conf
</pre>
	<p>Now you should have a functional cloud client. To begin, check out the help text and&nbsp;file.</p>
	<pre>
$ cd ../
</pre>
	<pre>
$ bin/cloud-client.sh --help

</pre>
	<h3>
		Check Your ssh Key</h3>
	<p>In order to use Nimbus clouds effectively, you need to have your ssh public key in a known place so that it can be injected into your VM, and thus allow you (and only you) root access to your VM. When creating your FutureGrid account you had to upload an ssh public key.&nbsp; That key can be found on hotel in the file <em>~/.ssh/authorized_keys</em>.&nbsp; If you were able to ssh into hotel then this is the public key are are currently using.&nbsp; Nimbus needs this key to be in the<em> ~/.ssh/id_rsa.pub </em>:</p>
	<pre>
$ cp ~/.ssh/authorized_keys ~/.ssh/id_rsa.pub
</pre>
	<p>Because the security environment can be complicated, cloud-client has an option to help verify that things are working.&nbsp; Run the following command to display some information about your security environment:</p>
	<pre>
$ ./bin/cloud-client.sh --security</pre>
	<h2>
		Using the Cloud Client</h2>
	<h3>
		Check out the various FutureGrid clouds</h3>
	<p>When the credentials file was untarred in a step above, a configuration file for each of the four FutureGrid clouds was put in your <em>conf/ </em>directory.&nbsp; Now let&#39;s take a look at accessing each of those clouds and seeing what virtual machines are available for use. This will require two options to cloud client.&nbsp; The first is --conf ; this is used to select the cloud you wish to use.&nbsp; Simply provide a path to the cloud configuration file.&nbsp; This --conf switch will be used in all commands to direct cloud-client at the cloud of interest.<br />
		<br />
		The second option is --list.&nbsp; This will simply provide a listing of all the available virtual machines:</p>
	<pre>
$ bin/cloud-client.sh --conf conf/hotel.conf --list
</pre>
	<p>This command should list the available images on the system.&nbsp; Notice the hello-cloud virtual machine.&nbsp; This is the test image we will use in this tutorial:</p>
	<pre>
[Image] &#39;hello-cloud&#39;                    Read only
        Modified: Jan 13 2011 @ 14:15   Size: 576716800 bytes (~550 MB)
</pre>
	<h3>
		Run a Virtual Machine</h3>
	Next, try to boot a virtual machine:<br />
	<pre>
$ bin/cloud-client.sh --conf conf/hotel.conf --run --name hello-cloud --hours 2
Launching workspace.
</pre>
	<pre>
Workspace Factory Service:
     https://svc.uc.futuregrid.org:8443/wsrf/services/WorkspaceFactoryService</pre>
	<pre>
&nbsp;</pre>
	<pre>
Creating workspace &quot;vm-001&quot;... done.</pre>
	<pre>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IP address:&nbsp;149.165.148.122&nbsp;
         Hostname: vm-148-122.uc.futuregrid.org       
       Start time: Wed Jul 25 15:44:33 CDT 2012
    Shutdown time: Wed Jul 25 17:44:33 CDT 2012
 Termination time: Wed Jul 25 17:46:33 CDT 2012</pre>
	<pre>
Waiting for updates.</pre>
	<pre>
&quot;vm-001&quot; reached target state: Running
</pre>
	<p>Once the image is running, you should be able to log into it with SSH. Note that you may need to wait another minute or so before you can actually get it, as the system needs time to boot and start services. Log in as the root user, and connect to the host printed out by the run command.&nbsp; Note that you <strong>must </strong>run this command in a location that has access to your private key.&nbsp; This means it must be in the <em>~/.ssh/</em> directory on the file system from where you launch this command, or you must have used the -A option to ssh when logging into hotel (as is shown above). Make sure you replace the hostname of the VM by the one printed out by cloud client.</p>
	<pre>
$ ssh root@vm-148-122.uc.futuregrid.org&nbsp;</pre>
	<h3>
		Create a New VM Image</h3>
	<p>Once you have a root shell on your VM, you may modify it as through it were a real machine.&nbsp; Here we encourage you to make some changes.&nbsp; Create a new user, install some additional software, or simply create a text file in the root user&#39;s account:</p>
	<pre>
# touch /root/CHANGE
# exit
</pre>
	<h3>
		Save the Changes to a New VM</h3>
	<p>Now that you have modified the VM, you can save it back into your personal repository.&nbsp; To do this, you will use the --save and --newname options.&nbsp; You will also need the VM handle as it was displayed in the output from the run command.&nbsp; If you have forgotten what this was, you can use the --status option to find it:</p>
	<pre>
$ ./bin/cloud-client.sh --conf conf/hotel.conf --status
Querying for ALL instances.

[*] - Workspace #32292. 149.165.148.253 [ vm-253.uc.futuregrid.org ]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; State: Running
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Duration: 120 minutes.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start time: Wed Jul 25 15:44:33 CDT 2012
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Shutdown time: Wed Jul 25 17:44:33 CDT 2012
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Termination time: Wed Jul 25 17:46:33 CDT 2012
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *Handle: vm-001
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Image: hello-cloud
</pre>
	<p>Note the handle <em>vm-001</em>.&nbsp; To save the VM for future use, run the following command:</p>
	<pre>
$ ./bin/cloud-client.sh --conf conf/hotel.conf --save --newname myvm --handle vm-001

Saving workspace.
  - Workspace handle (EPR): &#39;/N/u/bresnaha/nimbus-cloud-client-021/history/vm-001/vw-epr.xml&#39;
  - New name: &#39;myvm&#39;

Waiting for updates.

The image has successfully been transferred to your repository directory.

Finalizing the deployment now (terminating the resource lease).
</pre>
	<p>Do another listing of that cloud and you will see your VM is now available for launch:</p>
	<pre>
$ ./bin/cloud-client.sh --conf conf/hotel.conf --list
[Image] &#39;myvm&#39;                           Read/write
        Modified: Jul 25 2012 @ 20:49   Size: 576716800 bytes (~550 MB)

----

[Image] &#39;hello-cloud&#39;                    Read only
        Modified: Apr 8 2011 @ 13:56   Size: 576716800 bytes (~550 MB)
</pre>
	<h3>
		Launch Your New VM</h3>
	You can now launch your new VM just like you did the hello-cloud VM above, simply changing the name from <em>hello-cloud</em> to <em>myvm&nbsp;</em>:<br />
	<pre>
$ ./bin/cloud-client.sh --conf conf/hotel.conf --run --name myvm --hours 2

Launching workspace.

Workspace Factory Service:
&nbsp;&nbsp;&nbsp; <a href="https://svc.uc.futuregrid.org:8443/wsrf/services/WorkspaceFactoryService" title="https://svc.uc.futuregrid.org:8443/wsrf/services/WorkspaceFactoryService">https://svc.uc.futuregrid.org:8443/wsrf/services/WorkspaceFactoryService</a>

Creating workspace &quot;vm-002&quot;... done.


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; IP address: 149.165.148.122
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Hostname: vm-148-122.uc.futuregrid.org
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Start time: Wed Jul 25 15:58:31 CDT 2012
&nbsp;&nbsp;&nbsp; Shutdown time: Wed Jul 25 17:58:31 CDT 2012
&nbsp;Termination time: Wed Jul 25 18:08:31 CDT 2012

Waiting for updates.


&quot;vm-002&quot; reached target state: Running

Running: &#39;vm-002&#39;
</pre>
	<p>SSH into the machine and verify that your changes persisted.</p>
	<h3>
		Terminate the VM</h3>
	<p>Your VM will terminate after its allocated time expires.&nbsp; In our examples here, this is after 2 hours.&nbsp; However, you may wish to terminate it earlier.&nbsp; You can do so by again using the --handle option as you did in the <em>save a new VM step</em> and the --terminate option:</p>
	<pre>
$ ./bin/cloud-client.sh --conf conf/hotel.conf --terminate --handle vm-002

Terminating workspace.
  - Workspace handle (EPR): &#39;/N/u/bresnaha/nimbus-cloud-client-021/history/vm-002/vw-epr.xml&#39;

Destroying vm-002... destroyed.</pre>
	<p>&nbsp;</p>
	<h2>
		Virtual Clusters</h2>
	This is a basic walkthrough of how to run a sample virtual cluster.&nbsp; For more information on how they work, see <em>http://www.nimbusproject.org/docs/current/clouds/clusters2.html .</em><br />
	<pre>
</pre>
	<pre>
</pre>

<h3>
	Cluster Definition File</h3>
For this example, we will use a modification of the sample cluster file that is distributed with the cloud client.&nbsp; The file can be found at <em>https://portal.futuregrid.org/sites/default/files/tutorial-cluster.xml_.gz&nbsp;</em>. Copy the file to where your cloud-client program is located, and unzip it.&nbsp; Open the file and make note of the following:
<ol>
	<li>
		There are 2 workspace definitions.&nbsp;</li>
	<li>
		The head node has a quantity of 1 and a base image base-cluster-cc14.gz.&nbsp; It has the roles of providing a nfs server.&nbsp;</li>
	<li>
		The compute-nodes have the same image, but a quantity of 2.&nbsp; This means there will be 1 head node and 2 compute-nodes in the virtual cluster.&nbsp; This has the role of being a nfs client.</li>
</ol>
<h3>
	Start the Cluster</h3>
<pre>
$ ./bin/cloud-client.sh --conf conf/sierra.conf --run --hours 2 --cluster &lt;path to your cluster document&gt;
SSH known_hosts contained tilde:
&nbsp; - &#39;~/.ssh/known_hosts&#39; --&gt; &#39;/N/u/bresnaha/.ssh/known_hosts&#39;

Requesting cluster.
&nbsp; - head-node: image &#39;base-cluster-cc14.gz&#39;, 1 instance
&nbsp; - compute-nodes: image &#39;base-cluster-cc14.gz&#39;, 2 instances

Context Broker:
&nbsp;&nbsp;&nbsp; <a href="https://s83r.idp.sdsc.futuregrid.org:8443/wsrf/services/NimbusContextBroker" title="https://s83r.idp.sdsc.futuregrid.org:8443/wsrf/services/NimbusContextBroker">https://s83r.idp.sdsc.futuregrid.org:8443/wsrf/services/NimbusContextBroker</a>

Created new context with broker.

Workspace Factory Service:
&nbsp;&nbsp;&nbsp; <a href="https://s83r.idp.sdsc.futuregrid.org:8443/wsrf/services/WorkspaceFactoryService" title="https://s83r.idp.sdsc.futuregrid.org:8443/wsrf/services/WorkspaceFactoryService">https://s83r.idp.sdsc.futuregrid.org:8443/wsrf/services/WorkspaceFactory...</a>

Creating workspace &quot;head-node&quot;... done.
&nbsp; - 198.202.120.134 [ vm-40.sdsc.futuregrid.org ]

Creating group &quot;compute-nodes&quot;... done.
&nbsp; - 198.202.120.135 [ vm-41.sdsc.futuregrid.org ]
&nbsp; - 198.202.120.136 [ vm-42.sdsc.futuregrid.org ]

Launching cluster-004... done.

Waiting for launch updates.
&nbsp; - cluster-004: all members are Running
&nbsp; - wrote reports to &#39;/N/u/bresnaha/tutorial/nimbus-cloud-client-021/history/cluster-004/reports-vm&#39;

Waiting for context broker updates.
&nbsp; - cluster-004: contextualized
&nbsp; - wrote ctx summary to &#39;/N/u/bresnaha/tutorial/nimbus-cloud-client-021/history/cluster-004/reports-ctx/CTX-OK.txt&#39;
&nbsp; - wrote reports to &#39;/N/u/bresnaha/tutorial/nimbus-cloud-client-021/history/cluster-004/reports-ctx&#39;

SSH trusts new key for vm-40.sdsc.futuregrid.org&nbsp; <a href="http://en.wikipedia.org/wiki/ head-node "> head-node </a>

SSH trusts new key for vm-41.sdsc.futuregrid.org&nbsp; <a href="http://en.wikipedia.org/wiki/ compute-nodes #0 "> compute-nodes #0 </a>

SSH trusts new key for vm-42.sdsc.futuregrid.org&nbsp; <a href="http://en.wikipedia.org/wiki/ compute-nodes #1 "> compute-nodes #1 </a>
</pre>
This command takes a bit of time.&nbsp; What is happening is cloud-client is instructing Nimbus to start up three VMs on the user&#39;s behalf.&nbsp; Information is put into the context broker.&nbsp; When each VM boots, the context agent is run.&nbsp; The context agent checks in with the context broker and asks for information reflecting the <em>requires</em> section in the cluster document; similarly it registers its <em>provides</em> information with the context broker for other VM context agents to query.&nbsp; The NFS clients use this mechanism to provide the nfs server with their IP addresses.&nbsp; The NFS server then gets this information out of the context broker and uses it to authorize those IP addresses to remotely mount its disks.&nbsp;<br />
<br />
When it is complete, your virtual cluster will be ready to go.<br />
<h3>
	Check Out the Virtual Cluster</h3>
Now ssh into one of the worker nodes and check out the file system.<br />
<pre>
$ df -h
df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda1             2.9G  1.2G  1.7G  42% /
udev                  1.1G  144K  1.1G   1% /dev
shm                   1.1G     0  1.1G   0% /dev/shm
198.202.120.134:/home
                      2.9G  1.2G  1.7G  42% /home
198.202.120.134:/etc/grid-security/certificates
                      2.9G  1.2G  1.7G  42% /etc/grid-security/certificates</pre>
Notice the NFS mounted home directory. &nbsp;Touch a file in that directory.<br />
<pre>
$ echo &quot;Hello FutureGrid&quot; &gt; /home/test_file
</pre>
Now ssh into the other worker node and verify that the test file is visible to this node as well.<br />
<pre>
$cat /home/test_file
Hello FutureGrid
</pre>
<br />
<br />
<br /><table id="attachments" class="sticky-enabled">
 <thead><tr><th>Attachment</th><th>Size</th> </tr></thead>
<tbody>
 <tr class="odd"><td><a href="https://portal.futuregrid.org/sites/default/files/tutorial-cluster.xml_.gz">tutorial-cluster.xml_.gz</a></td><td>342 bytes</td> </tr>
</tbody>
</table>
  <div id="book-navigation-104" class="book-navigation">
    <ul class="menu"><li class="leaf cloud-quick-start-launch-a-vm-with-1-command first"><a href="/manual/nimbus/cloud-quick-start-launch-vm-1-command">Cloud Quick Start : Launch a VM with 1 command</a></li>
<li class="leaf futuregrid-tutorial-nm2-nimbus-one-click-cluster-guide last"><a href="/tutorials/nm2">FutureGrid Tutorial NM2 - Nimbus One-Click Cluster Guide </a></li>
</ul>
        <div class="page-links clear-block">
              <a href="/manual/openstack/grizzly" class="page-previous" title="Go to previous page">‹ OpenStack Grizzly on FutureGrid</a>
                    <a href="/manual/iaas" class="page-up" title="Go to parent page">up</a>
                    <a href="/manual/nimbus/cloud-quick-start-launch-vm-1-command" class="page-next" title="Go to next page">Cloud Quick Start : Launch a VM with 1 command ›</a>
          </div>
    
  </div>
  </div>

